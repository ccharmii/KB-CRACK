# -*- coding: utf-8 -*-
"""
ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ ë‰´ìŠ¤ ê·¼ê±° ë¶„ì„ ëª¨ë“ˆ (LangGraph ê¸°ë°˜)
í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì²´í¬ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + LLM) ì ìš©
"""

import json
import operator
import os
import numpy as np
from typing_extensions import TypedDict, Annotated, List, Dict, Any
from langchain_core.messages import BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_teddynote.tools.tavily import TavilySearch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# ìƒíƒœ ì •ì˜
class NewsAnalysisState(TypedDict):
    """ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œì˜ ìƒíƒœ"""
    # ì…ë ¥ ë°ì´í„°
    result_dir: Annotated[str, "ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ"]
    company_info: Annotated[Dict[str, Any], "íšŒì‚¬ ê¸°ë³¸ ì •ë³´"]
    anomalies: Annotated[List[Dict], "ë¶„ì„í•  ì´ìƒì¹˜ ëª©ë¡"]
    
    # ë¶„ì„ ê³¼ì • ë°ì´í„°
    current_anomaly: Annotated[Dict[str, Any], "í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ì´ìƒì¹˜"]
    current_anomaly_index: Annotated[int, "í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ì´ìƒì¹˜ ì¸ë±ìŠ¤"]
    search_query: Annotated[str, "í˜„ì¬ ê²€ìƒ‰ ì¿¼ë¦¬"]
    news_results: Annotated[List[Dict], "ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼"]
    relevance_scores: Annotated[List[float], "ê´€ë ¨ì„± ì ìˆ˜"]
    hybrid_relevance_data: Annotated[Dict, "í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ë¶„ì„ ê²°ê³¼"]
    retry_count: Annotated[int, "ì¬ê²€ìƒ‰ íšŸìˆ˜"]
    
    # ê²°ê³¼ ë°ì´í„°
    analysis_results: Annotated[List[Dict], "ë¶„ì„ ê²°ê³¼ ëª©ë¡"]
    
    # ë©”ì‹œì§€ì™€ ë‹¤ìŒ ë…¸ë“œ
    messages: Annotated[List[BaseMessage], operator.add]
    next_node: str

# LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
embedding_model = None

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ì´ˆê¸°í™”"""
    global embedding_model
    if embedding_model is None:
        try:
            embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        except Exception as e:
            print(f"    âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    return embedding_model

def create_reference_texts(company_name: str, metric_name: str, 
                         quarter: str, description: str) -> List[str]:
    """
    ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¸ì¡° í…ìŠ¤íŠ¸(ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ìš©, í•œêµ­ì–´ ë¬¸ìì—´)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    reference_texts = [
        f"{company_name} {metric_name} {quarter}",
        f"{company_name} {description}",
        f"{metric_name} ì´ìƒì¹˜ {quarter}",
        description,
        f"{company_name} ì¬ë¬´ì„±ê³¼ {quarter}",
        f"{company_name} {metric_name} ë³€í™”"
    ]
    
    # ì§€í‘œë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ê°€
    metric_keywords = {
        "ROE": ["ìê¸°ìë³¸ì´ìµë¥ ", "ìˆ˜ìµì„±", "ê²½ì˜íš¨ìœ¨ì„±", "íˆ¬ììˆ˜ìµ"],
        "Sales Growth Rate": ["ë§¤ì¶œ", "ìˆ˜ìµ", "ì„±ì¥", "íŒë§¤", "ì˜ì—…ì‹¤ì "],
        "PER": ["ì£¼ê°€ìˆ˜ìµë¹„ìœ¨", "ê¸°ì—…ê°€ì¹˜", "ì£¼ì‹", "íˆ¬ì", "ì‹œì¥í‰ê°€"],
        "Debt Ratio": ["ë¶€ì±„", "ì¬ë¬´ì•ˆì •ì„±", "ë ˆë²„ë¦¬ì§€", "ìë³¸êµ¬ì¡°"],
        "Current Ratio": ["ìœ ë™ì„±", "ë‹¨ê¸°ì§€ê¸‰ëŠ¥ë ¥", "í˜„ê¸ˆíë¦„"],
        "Operating Profit Margin": ["ì˜ì—…ì´ìµ", "ìˆ˜ìµì„±", "ë¹„ìš©ê´€ë¦¬", "ì‚¬ì—…ì„±ê³¼"]
    }
    
    if metric_name in metric_keywords:
        for keyword in metric_keywords[metric_name]:
            reference_texts.append(f"{company_name} {keyword}")
            reference_texts.append(f"{keyword} {quarter}")
    
    return reference_texts

def calculate_cosine_similarity_score(news_text: str, reference_texts: List[str]) -> float:
    """
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    """
    try:
        model = get_embedding_model()
        if model is None:
            return 5.0  # ê¸°ë³¸ê°’
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        news_embedding = model.encode([news_text])
        reference_embeddings = model.encode(reference_texts)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(news_embedding, reference_embeddings)[0]
        
        # ìµœëŒ€ ìœ ì‚¬ë„ë¥¼ 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        max_similarity = np.max(similarities)
        cosine_score = max_similarity * 10
        
        # numpy.float64ë¥¼ Python floatë¡œ ë³€í™˜
        return float(cosine_score)
    except Exception as e:
        print(f"    âš ï¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 5.0

def load_financial_data(state: NewsAnalysisState) -> NewsAnalysisState:
    """ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ë¬´ë¶„ì„ ê²°ê³¼ì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë¡œë“œ"""
    
    result_dir = state["result_dir"]
    
    # ì´ìƒì¹˜ ë°ì´í„° ë¡œë“œ
    anomaly_file = os.path.join(result_dir, "financial_anomalies.json")
    company_file = os.path.join(result_dir, "financial_analysis.json")

    try:
        # ì´ìƒì¹˜ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜
        if os.path.exists(anomaly_file):
            with open(anomaly_file, 'r', encoding='utf-8') as f:
                raw_anomaly_data = json.load(f)
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            anomalies = []
            if isinstance(raw_anomaly_data, dict):
                for metric_name, details in raw_anomaly_data.items():
                    if isinstance(details, dict):
                        anomaly_item = {
                            "metric_name": details.get("metric_name", metric_name),
                            "description": details.get("description", ""),
                            "severity": details.get("severity", "Medium"),
                            "quarter": details.get("quarter", "Latest"),
                            "type": details.get("type", ""),
                            "source": details.get("source", "")
                        }
                        anomalies.append(anomaly_item)
                    else:
                        anomaly_item = {
                            "metric_name": metric_name,
                            "description": str(details),
                            "severity": "Medium",
                            "quarter": "Latest",
                            "type": "unknown",
                            "source": "unknown"
                        }
                        anomalies.append(anomaly_item)
            elif isinstance(raw_anomaly_data, list):
                anomalies = raw_anomaly_data
            else:
                print(f"    âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì´ìƒì¹˜ ë°ì´í„° í˜•íƒœ: {type(raw_anomaly_data)}")
                anomalies = []
                
            print(f"    ğŸ“Š ì´ìƒì¹˜ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(anomalies)}ê°œ")
            for i, anomaly in enumerate(anomalies):
                print(f"        {i+1}. {anomaly['metric_name']}: {anomaly['description'][:50]}...")
        else:
            print(f"    âš ï¸ ì´ìƒì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {anomaly_file}")
            anomalies = []
        
        # íšŒì‚¬ ì •ë³´ ë¡œë“œ
        company_info = {}
        if os.path.exists(company_file):
            with open(company_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                company_info = data.get("ê¸°ì—…_ì •ë³´", {})
        
        # ìƒíƒœë¥¼ ì™„ì „íˆ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ë°˜í™˜
        new_state = dict(state)  # ê¸°ì¡´ ìƒíƒœ ë³µì‚¬
        if anomalies:
            new_state.update({
                "anomalies": anomalies,
                "company_info": company_info,
                "current_anomaly": anomalies[0],
                "current_anomaly_index": 0,
                "retry_count": 0,
                "analysis_results": [],
                "next_node": "generate_search_query"
            })
        else:
            new_state.update({
                "anomalies": [],
                "company_info": company_info,
                "analysis_results": [],
                "next_node": "save_results"
            })
        return new_state
            
    except Exception as e:
        print(f"    âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {str(e)}")
        new_state = dict(state)
        new_state.update({
            "anomalies": [],
            "company_info": {},
            "analysis_results": [],
            "next_node": "save_results"
        })
        return new_state

def generate_search_query(state: NewsAnalysisState) -> NewsAnalysisState:
    """ì´ìƒì¹˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
    
    query_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

    ## íšŒì‚¬ ì •ë³´
    íšŒì‚¬ëª…: {company_name}
    
    ## ì´ìƒì¹˜ ì •ë³´
    ì§€í‘œëª…: {metric_name}
    ë¶„ê¸°: {quarter}
    ì´ìƒì¹˜ ì„¤ëª…: {description}
    ì‹¬ê°ë„: {severity}
    
    ## ì¬ì‹œë„ ì •ë³´
    í˜„ì¬ ì‹œë„: {retry_count}íšŒ
    ì´ì „ ì¿¼ë¦¬: {previous_query}
    
    ## ìš”ì²­ì‚¬í•­
    - í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ì— ì í•©í•œ í‚¤ì›Œë“œë¡œ êµ¬ì„±
    - íšŒì‚¬ëª…ê³¼ ê´€ë ¨ ì¬ë¬´ì§€í‘œë¥¼ í¬í•¨
    - ì¬ì‹œë„ì¸ ê²½ìš° ë‹¤ë¥¸ ê°ë„ì˜ í‚¤ì›Œë“œ ì‚¬ìš©
    - 15ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
    
    ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    """)
    
    current_anomaly = state["current_anomaly"]
    company_info = state["company_info"]
    retry_count = state.get("retry_count", 0)
    
    company_name = company_info.get("company_name", company_info.get("ê¸°ì—…ëª…", ""))
    previous_query = state.get("search_query", "")
    
    try:
        chain = query_prompt | llm
        
        search_query = chain.invoke({
            "company_name": company_name,
            "metric_name": current_anomaly.get("metric_name", ""),
            "quarter": current_anomaly.get("quarter", ""),
            "description": current_anomaly.get("description", ""),
            "severity": current_anomaly.get("severity", ""),
            "retry_count": retry_count,
            "previous_query": previous_query
        }).content.strip()
        
        print(f"    ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {search_query}")
        
        new_state = dict(state)
        new_state.update({
            "search_query": search_query,
            "next_node": "search_news"
        })
        return new_state
        
    except Exception as e:
        print(f"    âŒ ì¿¼ë¦¬ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        new_state = dict(state)
        new_state.update({
            "search_query": f"{company_name} ì¬ë¬´ ë³€í™”",
            "next_node": "search_news"
        })
        return new_state

def search_news(state: NewsAnalysisState) -> NewsAnalysisState:
    """Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰"""
    
    search_query = state["search_query"]
    
    try:
        search_tool = TavilySearch(max_results=3, days=90)
        
        print(f"    ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: {search_query}")
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
        raw_results = search_tool.search(query=search_query)
        
        # ê²°ê³¼ ì •ë¦¬
        news_results = []
        if isinstance(raw_results, list):
            for result in raw_results:
                if isinstance(result, dict):
                    news_results.append({
                        "title": result.get("title", ""),
                        "url": result.get("url", ""),
                        "content": result.get("content", "")[:1000],
                        "published_date": result.get("published_date", ""),
                        "source": result.get("source", "")
                    })
        
        new_state = dict(state)
        new_state.update({
            "news_results": news_results,
            "next_node": "check_relevance_hybrid"
        })
        return new_state
        
    except Exception as e:
        print(f"    âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        new_state = dict(state)
        new_state.update({
            "news_results": [],
            "next_node": "check_relevance_hybrid"
        })
        return new_state

def check_relevance_hybrid(state: NewsAnalysisState) -> NewsAnalysisState:
    """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ê´€ë ¨ì„± ì²´í¬ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + LLM)"""
    
    semantic_analysis_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ ë‰´ìŠ¤ì™€ ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ ê°„ì˜ ì˜ë¯¸ì  ì—°ê´€ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

    ## ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ ì •ë³´
    íšŒì‚¬ëª…: {company_name}
    ì§€í‘œëª…: {metric_name}
    ë¶„ê¸°: {quarter}
    ì´ìƒì¹˜ ì„¤ëª…: {description}
    
    ## ë‰´ìŠ¤ ë‚´ìš©
    ì œëª©: {news_title}
    ë‚´ìš©: {news_content}
    
    ## ë¶„ì„ ìš”ì²­ì‚¬í•­
    1. ì¸ê³¼ê´€ê³„ ë¶„ì„: ì´ ë‰´ìŠ¤ê°€ í•´ë‹¹ ì¬ë¬´ì§€í‘œì— ì§ì ‘ì /ê°„ì ‘ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ê°€?
    2. ì‹œê°„ì  ì í•©ì„±: ë‰´ìŠ¤ ì‹œì ê³¼ ì¬ë¬´ì§€í‘œ ì¸¡ì • ì‹œì ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ê°€?
    3. ì‚¬ì—…ì˜ì—­ ì¼ì¹˜ì„±: ë‰´ìŠ¤ ë‚´ìš©ì´ í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ìš” ì‚¬ì—…ì˜ì—­ê³¼ ê´€ë ¨ìˆëŠ”ê°€?
    4. ì¬ë¬´ì˜í–¥ë„: ì´ ë‰´ìŠ¤ê°€ ì‹¤ì œë¡œ ì¬ë¬´ì„±ê³¼ì— ì¸¡ì • ê°€ëŠ¥í•œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ê°€?
    
    ë‹¤ìŒ ì˜ˆì‹œì²˜ëŸ¼ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                                                                
    {{
        "causal_relationship": {{
            "score": 8,
            "explanation": "ë‰´ìŠ¤ì—ì„œ ì–¸ê¸‰ëœ ì‹ ì œí’ˆ ì¶œì‹œê°€ ë§¤ì¶œ ì¦ê°€ì— ì§ì ‘ì  ì˜í–¥"
        }},
        "temporal_relevance": {{
            "score": 7,
            "explanation": "ë‰´ìŠ¤ ì‹œì ì´ í•´ë‹¹ ë¶„ê¸°ì™€ ì¼ì¹˜í•¨"
        }},
        "business_alignment": {{
            "score": 9,
            "explanation": "íšŒì‚¬ì˜ í•µì‹¬ ì‚¬ì—…ì˜ì—­ê³¼ ì™„ì „íˆ ì¼ì¹˜"
        }},
        "financial_impact": {{
            "score": 6,
            "explanation": "ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ì˜í–¥ ì œí•œì ì´ë‚˜ ì¤‘ì¥ê¸°ì  ì˜í–¥ ì˜ˆìƒ"
        }},
        "overall_semantic_score": 7.5,
        "reasoning": "ì¢…í•©ì ì¸ íŒë‹¨ ê·¼ê±°"
    }}
    """)
    
    current_anomaly = state["current_anomaly"]
    company_info = state["company_info"]
    news_results = state.get("news_results", [])
    
    if not news_results:
        new_state = dict(state)
        new_state.update({
            "relevance_scores": [],
            "hybrid_relevance_data": {},
            "next_node": "rewrite_query_or_analyze"
        })
        return new_state
    
    company_name = company_info.get("company_name", company_info.get("ê¸°ì—…ëª…", ""))
    metric_name = current_anomaly.get("metric_name", "")
    quarter = current_anomaly.get("quarter", "")
    description = current_anomaly.get("description", "")
    
    # ì°¸ì¡° í…ìŠ¤íŠ¸ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
    reference_texts = create_reference_texts(company_name, metric_name, quarter, description)
    
    # í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
    hybrid_results = []
    final_scores = []
    
    try:
        for i, news in enumerate(news_results):
            news_title = news.get("title", "")
            news_content = news.get("content", "")
            news_text = f"{news_title} {news_content}"
            
            # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            cosine_score = calculate_cosine_similarity_score(news_text, reference_texts)
            
            # 2. LLM ì˜ë¯¸ ë¶„ì„
            try:
                chain = semantic_analysis_prompt | llm | JsonOutputParser()
                
                semantic_analysis = chain.invoke({
                    "company_name": company_name,
                    "metric_name": metric_name,
                    "quarter": quarter,
                    "description": description,
                    "news_title": news_title,
                    "news_content": news_content
                })
                
                semantic_score = semantic_analysis.get("overall_semantic_score", 5.0)
                
            except Exception as e:
                print(f"    âš ï¸ LLM ì˜ë¯¸ ë¶„ì„ ì˜¤ë¥˜ (ë‰´ìŠ¤ {i+1}): {e}")
                semantic_analysis = {
                    "overall_semantic_score": 5.0,
                    "reasoning": f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
                }
                semantic_score = 5.0
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì½”ì‚¬ì¸ 30%, ì˜ë¯¸ ë¶„ì„ 70%)
            hybrid_score = (cosine_score * 0.3 + semantic_score * 0.7)
            hybrid_score = min(10.0, max(0.0, hybrid_score))
            
            # ê²°ê³¼ ì €ì¥ - ëª¨ë“  ìˆ«ìë¥¼ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            result = {
                "news_index": i,
                "cosine_similarity_score": float(round(cosine_score, 2)),
                "semantic_analysis_score": float(round(semantic_score, 2)),
                "hybrid_relevance_score": float(round(hybrid_score, 2)),
                "semantic_analysis": semantic_analysis,
                "news_info": {
                    "title": news_title,
                    "url": news.get("url", ""),
                    "published_date": news.get("published_date", "")
                }
            }
            
            hybrid_results.append(result)
            final_scores.append(float(hybrid_score))
        
        # ê²°ê³¼ ì •ë ¬ (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€)
        hybrid_results.sort(key=lambda x: x["hybrid_relevance_score"], reverse=True)
        
        # í†µê³„ ê³„ì‚°
        avg_hybrid_score = float(np.mean(final_scores)) if final_scores else 0.0
        max_hybrid_score = float(np.max(final_scores)) if final_scores else 0.0
        
        # ê´€ë ¨ì„± ê¸°ì¤€ (ì ìˆ˜ 6.0 ì´ìƒì„ ê´€ë ¨ìˆìŒìœ¼ë¡œ íŒë‹¨)
        relevant_threshold = 6.0
        has_relevant_news = any(score >= relevant_threshold for score in final_scores)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ë°ì´í„° êµ¬ì„±
        hybrid_relevance_data = {
            "analysis_method": "hybrid_cosine_llm",
            "weights": {"cosine": 0.3, "semantic": 0.7},
            "results": hybrid_results,
            "statistics": {
                "total_news_count": len(news_results),
                "average_hybrid_score": round(avg_hybrid_score, 2),
                "max_hybrid_score": round(max_hybrid_score, 2),
                "relevant_news_count": len([s for s in final_scores if s >= relevant_threshold])
            }
        }
        
        print(f"    ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì ìˆ˜: {avg_hybrid_score:.1f}/10 (ìµœê³ : {max_hybrid_score:.1f})")
        print(f"    - ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤: {len([s for s in final_scores if s >= relevant_threshold])}ê°œ")
        
        new_state = dict(state)
        new_state.update({
            "relevance_scores": final_scores,
            "hybrid_relevance_data": hybrid_relevance_data,
            "next_node": "rewrite_query_or_analyze" if not has_relevant_news else "analyze_news"
        })
        return new_state
        
    except Exception as e:
        print(f"    âŒ í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        new_state = dict(state)
        new_state.update({
            "relevance_scores": [],
            "hybrid_relevance_data": {},
            "next_node": "rewrite_query_or_analyze"
        })
        return new_state

def rewrite_query_or_analyze(state: NewsAnalysisState) -> NewsAnalysisState:
    """ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ì¿¼ë¦¬ë¥¼ ë‹¤ì‹œ ì‘ì„±í•˜ê±°ë‚˜ ë¶„ì„ ì§„í–‰"""
    
    retry_count = state.get("retry_count", 0)
    relevance_scores = state.get("relevance_scores", [])
    
    # ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì ìˆ˜ 6 ì´ìƒ)
    has_relevant = any(score >= 6 for score in relevance_scores) if relevance_scores else False
    
    new_state = dict(state)
    
    if not has_relevant and retry_count < 0: # 2ë¡œ ìˆ˜ì •
        # ì¬ê²€ìƒ‰
        print(f"    ğŸ”„ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì¬ê²€ìƒ‰ ({retry_count + 1}íšŒì°¨)")
        new_state.update({
            "retry_count": retry_count + 1,
            "next_node": "generate_search_query"
        })
    else:
        # ë¶„ì„ ì§„í–‰ (ì¬ì‹œë„ í•œê³„ ë„ë‹¬ ë˜ëŠ” ê´€ë ¨ ë‰´ìŠ¤ ë°œê²¬)
        new_state.update({
            "next_node": "analyze_news"
        })
    
    return new_state

def analyze_news(state: NewsAnalysisState) -> NewsAnalysisState:
    """ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì´ìƒì¹˜ ì›ì¸ì„ íŒŒì•…"""
    
    analysis_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ì˜ ì›ì¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

    ## ì´ìƒì¹˜ ì •ë³´
    ì§€í‘œëª…: {metric_name}
    ë¶„ê¸°: {quarter}
    ì´ìƒì¹˜ ì„¤ëª…: {description}
    ì‹¬ê°ë„: {severity}
    íšŒì‚¬ëª…: {company_name}

    ## ê´€ë ¨ ë‰´ìŠ¤ (í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì ìˆ˜ìˆœ)
    {relevant_news}

    ## í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì •ë³´
    - ë¶„ì„ ë°©ë²•: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (30%) + LLM ì˜ë¯¸ ë¶„ì„ (70%)
    - í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {avg_relevance_score}/10
    - ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤: {relevant_count}ê°œ

    ## ë¶„ì„ ìš”ì²­
    ìœ„ ë‰´ìŠ¤ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì´ìƒì¹˜ì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
    {{
        "primary_cause": "ì£¼ìš” ì›ì¸ (í•œì¤„ ìš”ì•½)",
        "confidence_level": 8,
        "supporting_evidence": ["ë‰´ìŠ¤ ê·¼ê±°1", "ë‰´ìŠ¤ ê·¼ê±°2", "ë‰´ìŠ¤ ê·¼ê±°3"],
        "detailed_explanation": "ìƒì„¸ ë¶„ì„ (3-4ë¬¸ì¥)",
        "news_sources": ["ë‰´ìŠ¤ URL1", "ë‰´ìŠ¤ URL2"],
        "impact_assessment": "ì˜í–¥ í‰ê°€",
        "risk_level": "ë‚®ìŒ/ë³´í†µ/ë†’ìŒ",
        "relevance_quality": "í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ì„ í†µí•œ ë‰´ìŠ¤ ê´€ë ¨ì„± í’ˆì§ˆ í‰ê°€"
    }}
    """)
    
    current_anomaly = state["current_anomaly"]
    company_info = state["company_info"]
    news_results = state.get("news_results", [])
    relevance_scores = state.get("relevance_scores", [])
    hybrid_relevance_data = state.get("hybrid_relevance_data", {})
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„±ì´ ë†’ì€ ë‰´ìŠ¤ë§Œ ì„ ë³„ (ì ìˆ˜ 5 ì´ìƒ)
    relevant_news = []
    if hybrid_relevance_data.get("results"):
        for result in hybrid_relevance_data["results"]:
            if result["hybrid_relevance_score"] >= 5.0:
                news_idx = result["news_index"]
                if news_idx < len(news_results):
                    news = news_results[news_idx]
                    relevant_news.append({
                        **news,
                        "hybrid_score": result["hybrid_relevance_score"],
                        "cosine_score": result["cosine_similarity_score"],
                        "semantic_score": result["semantic_analysis_score"]
                    })
    
    # ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
    if not relevant_news:
        # ë¹ˆ ë‰´ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ ì²˜ë¦¬
        relevant_news_text = "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        stats = {"average_hybrid_score": 0, "relevant_news_count": 0}
    else:
        # ë‰´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        news_text = []
        for i, news in enumerate(relevant_news):
            # ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ hybrid_score ê°€ì ¸ì˜¤ê¸°
            if isinstance(news, dict):
                hybrid_score = news.get("hybrid_score", "N/A")
                title = news.get("title", "")
                content = news.get("content", "")
                source = news.get("source", "")
                url = news.get("url", "")
            else:
                # newsê°€ dictê°€ ì•„ë‹Œ ê²½ìš° (ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©)
                hybrid_score = "N/A"
                title = str(news)
                content = ""
                source = ""
                url = ""
            
            text = f"[ë‰´ìŠ¤ {i+1}] (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {hybrid_score})\n"
            text += f"ì œëª©: {title}\n"
            text += f"ë‚´ìš©: {content}\n"
            text += f"ì¶œì²˜: {source}\n"
            text += f"URL: {url}\n"
            news_text.append(text)
        
        relevant_news_text = "\n---\n".join(news_text)
        stats = hybrid_relevance_data.get("statistics", {"average_hybrid_score": 0, "relevant_news_count": 0})
    
    # í†µê³„ ì •ë³´
    avg_relevance = stats.get("average_hybrid_score", 0)
    relevant_count = stats.get("relevant_news_count", 0)
    
    try:
        chain = analysis_prompt | llm | JsonOutputParser()
        
        analysis_result = chain.invoke({
            "metric_name": current_anomaly.get("metric_name", ""),
            "quarter": current_anomaly.get("quarter", ""),
            "description": current_anomaly.get("description", ""),
            "severity": current_anomaly.get("severity", ""),
            "company_name": company_info.get("company_name", company_info.get("ê¸°ì—…ëª…", "")),
            "relevant_news": relevant_news_text,
            "avg_relevance_score": avg_relevance,
            "relevant_count": relevant_count
        })
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        result_data = {
            "anomaly_info": current_anomaly,
            "analysis": analysis_result,
            "news_evidence": relevant_news,
            "hybrid_relevance_analysis": hybrid_relevance_data,
            "search_info": {
                "final_query": state.get("search_query", ""),
                "retry_count": state.get("retry_count", 0),
                "total_news_found": len(news_results),
                "relevant_news_count": len(relevant_news)
            },
            "timestamp": datetime.now().isoformat()
        }
        
        current_results = state.get("analysis_results", [])
        current_results.append(result_data)
        
        print(f"    âœ… í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ: {current_anomaly.get('metric_name', 'Unknown')}")
        
        new_state = dict(state)
        new_state.update({
            "analysis_results": current_results,
            "next_node": "check_remaining_anomalies"
        })
        return new_state
        
    except Exception as e:
        print(f"    âŒ ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        
        # ì˜¤ë¥˜ ê²°ê³¼ ì €ì¥
        error_result = {
            "anomaly_info": current_anomaly,
            "analysis": {"error": f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"},
            "news_evidence": relevant_news,
            "hybrid_relevance_analysis": hybrid_relevance_data,
            "timestamp": datetime.now().isoformat()
        }
        
        current_results = state.get("analysis_results", [])
        current_results.append(error_result)
        
        new_state = dict(state)
        new_state.update({
            "analysis_results": current_results,
            "next_node": "check_remaining_anomalies"
        })
        return new_state

def check_remaining_anomalies(state: NewsAnalysisState) -> NewsAnalysisState:
    """ë¶„ì„í•  ì´ìƒì¹˜ê°€ ë” ìˆëŠ”ì§€ í™•ì¸"""
    
    anomalies = state.get("anomalies", [])
    current_index = state.get("current_anomaly_index", 0)
    next_index = current_index + 1
    
    new_state = dict(state)
    
    if next_index < len(anomalies):
        # ë‹¤ìŒ ì´ìƒì¹˜ë¡œ ì´ë™
        print(f"    â¡ï¸ ë‹¤ìŒ ì´ìƒì¹˜ë¡œ ì´ë™ ({next_index + 1}/{len(anomalies)})")
        new_state.update({
            "current_anomaly": anomalies[next_index],
            "current_anomaly_index": next_index,
            "retry_count": 0,  # ì´ˆê¸°í™”
            "news_results": [],  # ì´ˆê¸°í™”
            "relevance_scores": [],  # ì´ˆê¸°í™”
            "hybrid_relevance_data": {},  # ì´ˆê¸°í™”
            "next_node": "generate_search_query"
        })
    else:
        # ëª¨ë“  ì´ìƒì¹˜ ë¶„ì„ ì™„ë£Œ
        print(f"    âœ… ëª¨ë“  ì´ìƒì¹˜ ë¶„ì„ ì™„ë£Œ ({len(anomalies)}ê°œ)")
        new_state.update({
            "next_node": "save_results"
        })
    
    return new_state

def save_results(state: NewsAnalysisState) -> NewsAnalysisState:
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    
    result_dir = state["result_dir"]
    analysis_results = state.get("analysis_results", [])
    company_info = state.get("company_info", {})
    
    # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
    final_results = {
        "success": True,
        "analysis_method": "hybrid_relevance_check",
        "methodology": {
            "cosine_similarity_weight": 0.3,
            "llm_semantic_weight": 0.7,
            "relevance_threshold": 6.0,
            "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2"
        },
        "company_info": company_info,
        "analysis_summary": {
            "total_anomalies": len(analysis_results),
            "successful_analyses": len([r for r in analysis_results if "error" not in r.get("analysis", {})]),
            "analysis_timestamp": datetime.now().isoformat()
        },
        "anomaly_news_analyses": analysis_results
    }
    
    # íŒŒì¼ ì €ì¥
    output_file = os.path.join(result_dir, "anomaly_news_analysis.json")
    
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(result_dir, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ’¾ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        new_state = dict(state)
        new_state.update({
            "next_node": "END"
        })
        return new_state
        
    except Exception as e:
        print(f"    âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        new_state = dict(state)
        new_state.update({
            "next_node": "END"
        })
        return new_state

def route_next_node(state: NewsAnalysisState) -> str:
    """ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    return state.get("next_node", "END")

def create_news_analysis_graph():
    """ë‰´ìŠ¤ ë¶„ì„ ê·¸ë˜í”„ë¥¼ ìƒì„±"""
    
    workflow = StateGraph(NewsAnalysisState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("load_financial_data", load_financial_data)
    workflow.add_node("generate_search_query", generate_search_query)
    workflow.add_node("search_news", search_news)
    workflow.add_node("check_relevance_hybrid", check_relevance_hybrid)
    workflow.add_node("rewrite_query_or_analyze", rewrite_query_or_analyze)
    workflow.add_node("analyze_news", analyze_news)
    workflow.add_node("check_remaining_anomalies", check_remaining_anomalies)
    workflow.add_node("save_results", save_results)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("load_financial_data")
    
    # ì—£ì§€ ì¶”ê°€
    workflow.add_conditional_edges(
        "load_financial_data",
        route_next_node,
        {
            "generate_search_query": "generate_search_query",
            "save_results": "save_results"
        }
    )
    
    workflow.add_edge("generate_search_query", "search_news")
    workflow.add_edge("search_news", "check_relevance_hybrid")
    
    workflow.add_conditional_edges(
        "check_relevance_hybrid",
        route_next_node,
        {
            "rewrite_query_or_analyze": "rewrite_query_or_analyze",
            "analyze_news": "analyze_news"
        }
    )
    
    workflow.add_conditional_edges(
        "rewrite_query_or_analyze",
        route_next_node,
        {
            "generate_search_query": "generate_search_query",
            "analyze_news": "analyze_news"
        }
    )
    
    workflow.add_edge("analyze_news", "check_remaining_anomalies")
    
    workflow.add_conditional_edges(
        "check_remaining_anomalies",
        route_next_node,
        {
            "generate_search_query": "generate_search_query",
            "save_results": "save_results"
        }
    )
    
    workflow.add_edge("save_results", END)
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory = MemorySaver()
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory)
    
    return app

def run_anomaly_news_analysis(result_dir: str) -> Dict[str, Any]:
    """
    ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ì— ëŒ€í•œ ë‰´ìŠ¤ ê·¼ê±° ë¶„ì„ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì²´í¬)
    
    Args:
        result_dir: ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
    """
    
    print("ğŸ“° ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ ë‰´ìŠ¤ ê·¼ê±° ë¶„ì„ ì‹œì‘... (í•˜ì´ë¸Œë¦¬ë“œ ê´€ë ¨ì„± ì²´í¬)")
    print("    - ë¶„ì„ ë°©ë²•: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (30%) + LLM ì˜ë¯¸ ë¶„ì„ (70%)")
    
    # ì„ë² ë”© ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    try:
        print("    ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        get_embedding_model()
        print("    âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        print(f"    âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("    ğŸ” ê¸°ë³¸ LLM ë¶„ì„ìœ¼ë¡œ ì§„í–‰...")
    
    # ê·¸ë˜í”„ ìƒì„±
    app = create_news_analysis_graph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "result_dir": result_dir,
        "messages": [],
        "analysis_results": [],
        "current_anomaly_index": 0,
        "retry_count": 0,
        "next_node": "load_financial_data"
    }
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    config = {
        "configurable": {
            "thread_id": f"news_analysis_hybrid_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        },
        "recursion_limit": 100
    }
    
    try:
        print("    ğŸš€ LangGraph ì‹¤í–‰ ì‹œì‘...")
        
        # ê·¸ë˜í”„ ì‹¤í–‰ with ë‹¨ê³„ë³„ ë””ë²„ê¹…
        final_state = None
        step_count = 0
        max_steps = 50
        
        for event in app.stream(initial_state, config):
            step_count += 1
            node_name = list(event.keys())[0]
            node_data = event[node_name]
            
            print(f"    ğŸ” ë‹¨ê³„ {step_count}: {node_name}")
            
            # ë‹¤ìŒ ë…¸ë“œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            next_node = node_data.get("next_node", "END")
            if next_node != "END":
                print(f"        â†’ ë‹¤ìŒ: {next_node}")
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_count >= max_steps:
                print(f"    âš ï¸ ìµœëŒ€ ë‹¨ê³„ ìˆ˜({max_steps}) ë„ë‹¬, ê°•ì œ ì¢…ë£Œ")
                break
                
            final_state = node_data
            
            # END ë…¸ë“œ ë„ë‹¬ ì‹œ ì¢…ë£Œ
            if next_node == "END":
                print("    âœ… ë¶„ì„ ì™„ë£Œ (END ë…¸ë“œ ë„ë‹¬)")
                break
        
        if final_state is None:
            print("    âš ï¸ final_stateê°€ None, app.get_state()ë¡œ ìƒíƒœ ì¡°íšŒ")
            final_state = app.get_state(config).values
        
        analysis_results = final_state.get("analysis_results", [])
        
        # í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ í†µê³„ ê³„ì‚°
        total_news_analyzed = 0
        avg_hybrid_scores = []
        
        for result in analysis_results:
            hybrid_data = result.get("hybrid_relevance_analysis", {})
            stats = hybrid_data.get("statistics", {})
            
            if stats:
                total_news_analyzed += stats.get("total_news_count", 0)
                avg_score = stats.get("average_hybrid_score", 0)
                if avg_score > 0:
                    avg_hybrid_scores.append(float(avg_score))
        
        overall_avg_score = float(np.mean(avg_hybrid_scores)) if avg_hybrid_scores else 0.0
        
        return {
            "success": True,
            "analysis_method": "hybrid_relevance_check",
            "total_anomalies_analyzed": len(analysis_results),
            "total_news_analyzed": total_news_analyzed,
            "average_relevance_score": round(overall_avg_score, 2),
            "analysis_results": analysis_results,
            "output_file": os.path.join(result_dir, "anomaly_news_analysis.json")
        }
        
    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "total_anomalies_analyzed": 0,
            "analysis_method": "hybrid_relevance_check"
        }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_result_dir = "analysis_results/20250812_123456"  # ì˜ˆì‹œ ê²½ë¡œ
    
    result = run_anomaly_news_analysis(test_result_dir)

    print("\n=== í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ ===")
    print(f"ì„±ê³µ ì—¬ë¶€: {result['success']}")
    print(f"ë¶„ì„ ë°©ë²•: {result.get('analysis_method', 'N/A')}")
    print(f"ë¶„ì„ëœ ì´ìƒì¹˜ ìˆ˜: {result['total_anomalies_analyzed']}")
    
    if result['success']:
        print(f"ì´ ë¶„ì„ëœ ë‰´ìŠ¤ ìˆ˜: {result.get('total_news_analyzed', 0)}")
        print(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {result.get('average_relevance_score', 0)}/10")
        print(f"ê²°ê³¼ íŒŒì¼: {result['output_file']}")
    
    if result.get('error'):
        print(f"ì˜¤ë¥˜: {result['error']}")