import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field


class NewsRiskItem(BaseModel):
    """ë‰´ìŠ¤ ìœ„í—˜ í•­ëª© ëª¨ë¸"""
    title: str = Field(description="ë‰´ìŠ¤ ì œëª©")
    content_summary: str = Field(description="ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½ (1-2ë¬¸ì¥)")
    url: str = Field(description="ë‰´ìŠ¤ URL")
    risk_category: str = Field(description="ìœ„í—˜ ì¹´í…Œê³ ë¦¬ (ì¬ë¬´ìœ„í—˜/ë¹„ì¬ë¬´ìœ„í—˜/ë³µí•©ìœ„í—˜)")
    risk_level: int = Field(description="ìœ„í—˜ë„ (1-10, 10ì´ ê°€ì¥ ìœ„í—˜)")
    evidence_basis: str = Field(description="ìœ„í—˜ íŒë‹¨ ê·¼ê±°")
    anomaly_connection: str = Field(description="íƒì§€ëœ ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„±")
    published_date: str = Field(description="ë°œí–‰ì¼", default="")


class CreditAssessment(BaseModel):
    """ì‹ ìš©ë“±ê¸‰ í‰ê°€ ëª¨ë¸"""
    current_grade: str = Field(description="í˜„ì¬ ì¶”ì • ì‹ ìš©ë“±ê¸‰")
    predicted_grade: str = Field(description="ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰")
    change_probability: float = Field(description="ë“±ê¸‰ ë³€ê²½ í™•ë¥  (0-1)")
    change_direction: str = Field(description="ë³€ê²½ ë°©í–¥ (ìƒìŠ¹/í•˜ë½/ìœ ì§€)")
    reasoning: str = Field(description="ë“±ê¸‰ í‰ê°€ ê·¼ê±°")


class DailyRiskSummary(BaseModel):
    """ì¼ì¼ ìœ„í—˜ ìš”ì•½ ëª¨ë¸"""
    analysis_date: str = Field(description="ë¶„ì„ ë‚ ì§œ")
    company_name: str = Field(description="ê¸°ì—…ëª…")
    total_risk_news: int = Field(description="ìœ„í—˜ ë‰´ìŠ¤ ì´ ê°œìˆ˜")
    high_risk_count: int = Field(description="ê³ ìœ„í—˜ ë‰´ìŠ¤ ê°œìˆ˜ (ìœ„í—˜ë„ 7-10)")
    medium_risk_count: int = Field(description="ì¤‘ìœ„í—˜ ë‰´ìŠ¤ ê°œìˆ˜ (ìœ„í—˜ë„ 4-6)")
    key_risk_summary: str = Field(description="ì£¼ìš” ìœ„í—˜ ìš”ì•½ (2-3ë¬¸ì¥)")
    risk_news_items: List[NewsRiskItem] = Field(description="ìœ„í—˜ ë‰´ìŠ¤ ëª©ë¡")
    credit_assessment: CreditAssessment = Field(description="ì‹ ìš©ë“±ê¸‰ í‰ê°€")


class NewsRelevance(BaseModel):
    """ë‰´ìŠ¤ ê´€ë ¨ì„± íŒë‹¨ ëª¨ë¸"""
    is_relevant: bool = Field(description="ë‰´ìŠ¤ê°€ ì‹ ìš©ìœ„í—˜ ë¶„ì„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€")
    reason: str = Field(description="ê´€ë ¨ì„± íŒë‹¨ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±°")


class CreditRiskNewsAnalyzer:
    def __init__(self, max_search_results=10, openai_api_key=None, tavily_api_key=None):
        """
        ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            max_search_results (int): ë‰´ìŠ¤ ê²€ìƒ‰ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            openai_api_key (str, optional): OpenAI API í‚¤
            tavily_api_key (str, optional): Tavily API í‚¤
        """
        # API í‚¤ ì„¤ì •
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        if tavily_api_key:
            os.environ["TAVILY_API_KEY"] = tavily_api_key
        
        self.tavily_search = TavilySearch(max_results=max_search_results)
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
        
        # ì‹ ìš©ìœ„í—˜ ì§•í›„ ê¸°ì¤€ ì •ì˜
        self.credit_risk_criteria = {
            "ì¬ë¬´ ìœ„í—˜ ì§•í›„": {
                "ìˆ˜ìµì„± ì•…í™”": ["ë§¤ì¶œ ê°ì†Œ", "ì´ìµë¥  í•˜ë½", "ì†ì‹¤ í™•ëŒ€", "ì˜ì—…ì ì"],
                "ì¬ë¬´êµ¬ì¡° ì•…í™”": ["ë¶€ì±„ ì¦ê°€", "ì°¨ì…ê¸ˆ ì¦ê°€", "ì‹ ìš©ë“±ê¸‰ í•˜ë½", "ê¸ˆìœµë¹„ìš© ì¦ê°€"],
                "ìœ ë™ì„± ìœ„ê¸°": ["í˜„ê¸ˆ ë¶€ì¡±", "ìœ ë™ì„± ê²½ìƒ‰", "ìê¸ˆì¡°ë‹¬ ì–´ë ¤ì›€", "ìš´ì „ìê¸ˆ ë¶€ì¡±"],
                "íˆ¬ì ëŠ¥ë ¥ ì €í•˜": ["íˆ¬ì ì¶•ì†Œ", "ì„¤ë¹„íˆ¬ì ê°ì†Œ", "ì—°êµ¬ê°œë°œë¹„ ì‚­ê°", "ì‹ ê·œíˆ¬ì ì¤‘ë‹¨"]
            },
            "ë¹„ì¬ë¬´ ìœ„í—˜ ì§•í›„": {
                "ê±°ë²„ë„ŒìŠ¤ ì´ìŠˆ": ["ì§€ë°°êµ¬ì¡° ë¬¸ì œ", "ê²½ì˜ì§„ êµì²´", "ë‚´ë¶€ ê°ˆë“±", "ì£¼ì£¼ ë¶„ìŸ"],
                "ê·œì œ/ë²•ì  ë¦¬ìŠ¤í¬": ["ê·œì œ ìœ„ë°˜", "ê³¼ì§•ê¸ˆ", "ì†Œì†¡", "ë²•ì  ë¶„ìŸ", "ì œì¬ ì¡°ì¹˜"],
                "ìš´ì˜ ë¦¬ìŠ¤í¬": ["ìƒì‚° ì¤‘ë‹¨", "ê³µê¸‰ë§ ì°¨ì§ˆ", "í’ˆì§ˆ ë¬¸ì œ", "ì‚¬ê³  ë°œìƒ"],
                "ESG ë¦¬ìŠ¤í¬": ["í™˜ê²½ ì˜¤ì—¼", "ì‚¬íšŒì  ë¬¼ì˜", "ë…¸ì‚¬ ê°ˆë“±", "í‰íŒ ì•…í™”"],
                "ì‹œì¥ ë¦¬ìŠ¤í¬": ["ì‹œì¥ì ìœ ìœ¨ í•˜ë½", "ê²½ìŸ ì‹¬í™”", "ê³ ê° ì´íƒˆ", "ë¸Œëœë“œ ê°€ì¹˜ í•˜ë½"]
            }
        }

    def load_analysis_results(self, result_dir: str) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ë¬´ë¶„ì„ê³¼ ë¹„ì¬ë¬´ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œë“œ
        
        Args:
            result_dir (str): ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            Dict: í†µí•©ëœ ë¶„ì„ ê²°ê³¼
        """
        analysis_data = {
            "company_name": "",
            "current_credit_grade": "A",  # ê¸°ë³¸ê°’
            "financial_anomalies": [],
            "non_financial_anomalies": [],
            "financial_characteristics": {},
            "business_context": {}
        }
        
        # 1. ì¬ë¬´ ì´ìƒì¹˜ ê²°ê³¼ ë¡œë“œ (financial_anomalies.json)
        financial_anomalies_file = os.path.join(result_dir, "financial_anomalies.json")
        if os.path.exists(financial_anomalies_file):
            try:
                with open(financial_anomalies_file, 'r', encoding='utf-8') as f:
                    raw_financial_anomalies = json.load(f)
                
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                financial_anomalies = []
                if isinstance(raw_financial_anomalies, dict):
                    for metric_name, details in raw_financial_anomalies.items():
                        if isinstance(details, dict):
                            anomaly_item = {
                                "indicator": details.get("metric_name", metric_name),
                                "severity": details.get("severity", "medium").lower(),
                                "description": details.get("description", ""),
                                "type": details.get("type", ""),
                                "quarter": details.get("quarter", ""),
                                "source": details.get("source", ""),
                                "metric_name": details.get("metric_name", metric_name)
                            }
                            financial_anomalies.append(anomaly_item)
                        else:
                            # fallback: ê°„ë‹¨í•œ í˜•íƒœë¡œ ë³€í™˜
                            anomaly_item = {
                                "indicator": metric_name,
                                "severity": "medium",
                                "description": str(details),
                                "type": "unknown",
                                "quarter": "Latest",
                                "source": "unknown",
                                "metric_name": metric_name
                            }
                            financial_anomalies.append(anomaly_item)
                elif isinstance(raw_financial_anomalies, list):
                    # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                    for anomaly in raw_financial_anomalies:
                        financial_anomalies.append({
                            "indicator": anomaly.get("metric_name", ""),
                            "severity": anomaly.get("severity", "medium").lower(),
                            "description": anomaly.get("description", ""),
                            "type": anomaly.get("type", ""),
                            "quarter": anomaly.get("quarter", ""),
                            "source": anomaly.get("source", "")
                        })
                
                analysis_data["financial_anomalies"] = financial_anomalies
                print(f"âœ… ì¬ë¬´ ì´ìƒì¹˜ ë¡œë“œ ì™„ë£Œ: {len(financial_anomalies)}ê°œ")
                
            except Exception as e:
                print(f"âŒ ì¬ë¬´ ì´ìƒì¹˜ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        
        # 2. ê¸°ì—… ì •ë³´ ì¶”ì¶œ
        financial_analysis_file = os.path.join(result_dir, "financial_analysis.json")
        if os.path.exists(financial_analysis_file):
            try:
                with open(financial_analysis_file, 'r', encoding='utf-8') as f:
                    financial_data = json.load(f)
                
                # ê¸°ì—… ì •ë³´ ì¶”ì¶œ
                company_info = financial_data.get("ê¸°ì—…_ì •ë³´", {})
                analysis_data["company_name"] = company_info.get("ê¸°ì—…ëª…", "Unknown")
                analysis_data["current_credit_grade"] = company_info.get("Current_credit_grade", "A")
                
                # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
                analysis_data["business_context"] = {
                    "company_info": company_info,
                    "industry": company_info.get("ì—…ì¢…", ""),
                    "business_area": company_info.get("ì œí’ˆêµ°", "")
                }
                
                print(f"âœ… ê¸°ì—… ì •ë³´ ë¡œë“œ ì™„ë£Œ: {analysis_data['company_name']}")
                
            except Exception as e:
                print(f"âŒ ê¸°ì—… ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
                # ê¸°ë³¸ê°’ ì„¤ì •
                analysis_data["company_name"] = "ì‚¼ì„±ì „ì"
        
        # 3. ë¹„ì¬ë¬´ë¶„ì„ ê²°ê³¼ ë¡œë“œ (non_financial_reasoning.json)
        nfr_file = os.path.join(result_dir, "non_financial_reasoning.json")
        if os.path.exists(nfr_file):
            try:
                with open(nfr_file, 'r', encoding='utf-8') as f:
                    nfr_data = json.load(f)
                
                # JSON êµ¬ì¡° í™•ì¸: results ë°°ì—´ì´ ìˆëŠ”ì§€ ì²´í¬
                if nfr_data.get("success") and "results" in nfr_data:
                    corp_code = nfr_data.get("corp_code", "")
                    quarter = nfr_data.get("quarter", "")
                    
                    # results ë°°ì—´ì˜ ê° í•­ëª©ì„ ì²˜ë¦¬
                    for result in nfr_data["results"]:
                        metric = result.get("metric", "")
                        anomaly_text = result.get("anomaly_text", "")
                        explanation = result.get("explanation_ko", "")
                        confidence = result.get("confidence", 0)
                        
                        # ì‹¬ê°ë„ ê³„ì‚°
                        severity_text = result.get("severity", "Medium")
                        severity = severity_text.lower() if severity_text in ["High", "Medium", "Low"] else "medium"
                        
                        # ë¶„ì„ ë°ì´í„°ì— ì¶”ê°€
                        analysis_data["non_financial_anomalies"].append({
                            "indicator": metric,
                            "pillar": "Unknown",
                            "score": 0,
                            "grade_label": anomaly_text,
                            "severity": severity,
                            "description": explanation,
                            "confidence": confidence,
                            "quarter": quarter,
                            "indicator_id": metric
                        })
                    
                    print(f"âœ… ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(analysis_data['non_financial_anomalies'])}ê°œ í•­ëª©")
                    
                else:
                    print(f"âŒ ë¹„ì¬ë¬´ë¶„ì„ JSON êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„: success={nfr_data.get('success')}")
                    
            except Exception as e:
                print(f"âŒ ë¹„ì¬ë¬´ë¶„ì„ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        
        # 4. ì¬ë¬´ íŠ¹ì„± ë¶„ì„
        analysis_data["financial_characteristics"] = self._analyze_financial_characteristics(analysis_data["financial_anomalies"])
        
        return analysis_data

    def _analyze_financial_characteristics(self, anomalies: List[Dict]) -> Dict[str, Any]:
        """ì¬ë¬´ ì´ìƒì¹˜ì—ì„œ íŠ¹ì„±ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        characteristics = {
            "severity_distribution": {"high": 0, "medium": 0, "low": 0},
            "type_distribution": {"peer_comparison": 0, "time_series": 0, "other": 0},
            "indicator_types": {},
            "quarters_affected": set(),
            "risk_indicators": [],
            "total_anomalies": len(anomalies)
        }
        
        for anomaly in anomalies:
            # ì‹¬ê°ë„ ë¶„í¬
            severity = anomaly.get("severity", "medium").lower()
            if severity in characteristics["severity_distribution"]:
                characteristics["severity_distribution"][severity] += 1
            
            # ìœ í˜• ë¶„í¬
            anomaly_type = anomaly.get("type", "other")
            if anomaly_type in characteristics["type_distribution"]:
                characteristics["type_distribution"][anomaly_type] += 1
            else:
                characteristics["type_distribution"]["other"] += 1
            
            # ì§€í‘œ ìœ í˜• ë¶„í¬
            indicator = anomaly.get("indicator", "")
            if indicator:
                characteristics["indicator_types"][indicator] = characteristics["indicator_types"].get(indicator, 0) + 1
            
            # ì˜í–¥ë°›ì€ ë¶„ê¸°
            quarter = anomaly.get("quarter", "")
            if quarter:
                characteristics["quarters_affected"].add(quarter)
            
            # ê³ ìœ„í—˜ ì§€í‘œ
            if severity == "high":
                characteristics["risk_indicators"].append(indicator)
        
        # setì„ listë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
        characteristics["quarters_affected"] = list(characteristics["quarters_affected"])
        
        return characteristics
    
    def _generate_augmented_queries(self, company_name) -> List[str]:
        """
        íšŒì‚¬ ì´ë¦„ê³¼ ë‹¤ì–‘í•œ ìœ„í—˜ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        base_query = f'"{company_name}"' # ì •í™•í•œ íšŒì‚¬ëª… ê²€ìƒ‰ì„ ìœ„í•´ í°ë”°ì˜´í‘œ ì‚¬ìš©

        # ì¬ë¬´ì  ìœ„í—˜ í‚¤ì›Œë“œ
        financial_risk_keywords = [
            "ìœ ë™ì„± ìœ„ê¸°", "ìê¸ˆë‚œ", "ë¶€ì±„ ê³¼ë‹¤", "ì˜ì—…ì†ì‹¤", "ì ì ì§€ì†",
            "ì‹ ìš©ë“±ê¸‰ í•˜í–¥", "ì±„ë¬´ ë¶ˆì´í–‰", "ì›Œí¬ì•„ì›ƒ", "ë²•ì •ê´€ë¦¬",
            "ê°ì‚¬ì˜ê²¬ ê±°ì ˆ", "ìë³¸ì ì‹", "ì–´ë‹ì‡¼í¬"
        ]

        # ë¹„ì¬ë¬´ì  ìœ„í—˜ í‚¤ì›Œë“œ
        non_financial_risk_keywords = [
            "íš¡ë ¹", "ë°°ì„", "ë¶„ì‹íšŒê³„", "ì†Œì†¡", "ê³µì •ìœ„ ì¡°ì‚¬", "ì••ìˆ˜ìˆ˜ìƒ‰",
            "ì˜ì—…ì •ì§€", "ì‚¬ì—… ì¤‘ë‹¨", "ê²½ì˜ì§„ ì‚¬í‡´", "êµ¬ì¡°ì¡°ì •"
        ]

        # ê¸°ë³¸ ì¿¼ë¦¬ ì¶”ê°€
        queries = [f'{base_query} ì‹ ìš©ìœ„í—˜', f'{base_query} ë¶€ë„ ê°€ëŠ¥ì„±']

        # í‚¤ì›Œë“œ ì¡°í•© ì¿¼ë¦¬ ì¶”ê°€
        for keyword in financial_risk_keywords:
            queries.append(f'{base_query} {keyword}')

        for keyword in non_financial_risk_keywords:
            queries.append(f'{base_query} {keyword}')

        return list(set(queries)) # ì¤‘ë³µ ì¿¼ë¦¬ ì œê±°

    def search_targeted_news(self, analysis_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ë¶„ì„ëœ ì´ìƒì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ì œì™€ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ë¥¼ íƒ€ê²ŸíŒ…í•˜ì—¬ ê²€ìƒ‰
        
        Args:
            analysis_data: ì¬ë¬´/ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼
            
        Returns:
            List[Dict]: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        company_name = analysis_data.get("company_name", "ì‚¼ì„±ì „ì")
        financial_anomalies = analysis_data.get("financial_anomalies", [])
        nfr_anomalies = analysis_data.get("non_financial_anomalies", [])
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±


        print("ğŸ” ì¦ê°•ëœ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        search_keywords = self._generate_augmented_queries(company_name)
        print(f"  - ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(search_keywords)}ê°œ")
        
        search_keywords.append( f"{company_name} ë‰´ìŠ¤")

        seen_urls = set()
        all_news = []

        for keyword in search_keywords:
            try:

                results = self.tavily_search.search(
                    query=keyword,
                    search_depth="advanced",
                    # topic="news",  # <-- ì´ ë¶€ë¶„ì„ ì¶”ê°€í•˜ì—¬ ë‰´ìŠ¤ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í•œì •í•©ë‹ˆë‹¤.
                    include_answer=False,
                    include_raw_content=False,
                    max_results=5,
                    # days=2  # ë‚ ì§œ ë²”ìœ„ í•„í„°: ì–´ì œë¶€í„° ì˜¤ëŠ˜ê¹Œì§€
                )
                
                if results:
                    if isinstance(results, list):
                        for result in results:
                            news_item = self._process_search_result_clean(result, keyword)
                            if news_item:
                                all_news.append(news_item)
                    else:
                        news_item = self._process_search_result_clean(results, keyword)
                        if news_item:
                            all_news.append(news_item)
                            
            except Exception as e:
                print(f"   - ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_news = []
        for news in all_news:
            url = news.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(news)
        
        return unique_news
    
    def _pre_filter_relevant_news(self, news_list: List[Dict[str, Any]], analysis_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡ì„ ì‚¬ì „ í•„í„°ë§í•˜ì—¬ ì‹ ìš©ìœ„í—˜ê³¼ ê´€ë ¨ ìˆëŠ” ë‰´ìŠ¤ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.

        Args:
            news_list (List[Dict[str, Any]]): ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡
            analysis_data (Dict[str, Any]): ë¶„ì„ ë°ì´í„°

        Returns:
            List[Dict[str, Any]]: í•„í„°ë§ëœ ê´€ë ¨ ë‰´ìŠ¤ ëª©ë¡
        """
        if not news_list:
            return []

        print("   - ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§ ì‹œì‘...")
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """
You are a financial analyst's assistant. Your task is to quickly determine if a news article is potentially relevant for a company's credit risk assessment.
You will be given the company's name, a summary of detected financial and non-financial anomalies, and general credit risk definitions.
Based on this context, evaluate the provided news article. If the news article does not contain anomalies, but you determine it is related to the detected financial or non-financial anomalies, evaluate it as an anomaly.
Respond ONLY with a JSON object with two keys: "is_relevant" (boolean) and "reason" (a brief one-sentence explanation).
"""),
            ("human", """
**Context for Assessment**
- Company Name: {company_name}
- Detected Financial Anomalies Summary: {financial_anomalies_summary}
- Detected Non-Financial Anomalies Summary: {non_financial_anomalies_summary}
- General Credit Risk Criteria: {credit_risk_criteria}D

**News Article to Evaluate**
- Title: {news_title}
- Content: {news_content}

Is this news article potentially relevant for a credit risk assessment?
Provide your answer in JSON format.
""")
        ])

        relevance_chain = prompt_template | self.llm | JsonOutputParser(pydantic_object=NewsRelevance)

        relevant_news = []
        # í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ì •ë³´ ìƒì„±
        financial_anomalies = analysis_data.get("financial_anomalies", [])
        nfr_anomalies = analysis_data.get("non_financial_anomalies", [])

        financial_summary = f"{len(financial_anomalies)} financial anomalies detected. High severity indicators include: {', '.join([a['indicator'] for a in financial_anomalies if a.get('severity') == 'high'][:3])}"
        nfr_summary = f"{len(nfr_anomalies)} non-financial anomalies detected."
        risk_criteria_summary = "Keywords include: revenue decline, debt increase, legal disputes, governance issues, market share loss."

        for news in news_list:
            try:
                result = relevance_chain.invoke({
                    "company_name": analysis_data.get("company_name"),
                    "financial_anomalies_summary": financial_summary,
                    "non_financial_anomalies_summary": nfr_summary,
                    "credit_risk_criteria": risk_criteria_summary,
                    "news_title": news.get("title"),
                    "news_content": news.get("content", "")[:800]  # ë¹ ë¥¸ íŒë‹¨ì„ ìœ„í•´ ë‚´ìš© ê¸¸ì´ ì œí•œ
                })

                if isinstance(result, dict) and result.get("is_relevant"):
                    print(f"      - âœ… RELEVANT: {news.get('title')}")
                    relevant_news.append(news)
                else:
                    print(f"      - âŒ IRRELEVANT: {news.get('title')}")

            except Exception as e:
                print(f"      - âš ï¸ Pre-filtering error for news '{news.get('title')}': {e}")
                continue
                
        print(f"   - ì‚¬ì „ í•„í„°ë§ ì™„ë£Œ: {len(relevant_news)} / {len(news_list)} ë‰´ìŠ¤ ì„ ë³„")
        return relevant_news

    def _process_search_result_clean(self, result: Any, keyword: str) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•˜ì—¬ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            result: ê²€ìƒ‰ ê²°ê³¼ (dict ë˜ëŠ” str)
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            
        Returns:
            Dict: ì •ë¦¬ëœ ë‰´ìŠ¤ ì•„ì´í…œ
        """
        try:
            if isinstance(result, dict):
                # ì œëª© ì •ë¦¬
                title = result.get('title', 'ì œëª© ì—†ìŒ')
                title = self._clean_title(title)
                
                # ë‚´ìš© ì •ë¦¬
                content = result.get('content', result.get('snippet', ''))
                content = self._clean_content(content)
                
                # URL ê²€ì¦
                url = result.get('url', '')
                
                # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                if len(content) < 30:
                    return None
                
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_date': result.get('published_date', result.get('date', '')),
                    'search_keyword': keyword,
                    'search_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            else:
                return None
                
        except Exception as e:
            print(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _clean_title(self, title: str) -> str:
        """ë‰´ìŠ¤ ì œëª© ì •ë¦¬"""
        if not title:
            return "ì œëª© ì—†ìŒ"
        
        import re
        title = re.sub(r'<[^>]+>', '', title)
        title = title.replace('\n', ' ').replace('\t', ' ')
        title = re.sub(r'\s+', ' ', title).strip()
        
        if len(title) > 100:
            title = title[:100] + "..."
        
        return title
    
    def _clean_content(self, content: str) -> str:
        """ë‰´ìŠ¤ ë‚´ìš© ì •ë¦¬"""
        if not content:
            return "ë‚´ìš© ì—†ìŒ"
        
        import re
        
        content = re.sub(r'<[^>]+>', '', content)
        content = re.sub(r'', '', content, flags=re.DOTALL)
        content = re.sub(r'javascript:[^"\']*', '', content)
        content = re.sub(r'https?://[^\s]+', '', content)
        
        unwanted_patterns = [
            r'ì´ë¯¸ì§€.*?jpg|png|gif',
            r'ê²€ìƒ‰.*?í•˜ê¸°',
            r'í˜ì´ì§€.*?ë§í¬',
            r'Copyright.*?\d{4}',
            r'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸.*?\d',
        ]
        
        for pattern in unwanted_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        content = content.replace('\n', ' ').replace('\t', ' ')
        content = re.sub(r'\s+', ' ', content).strip()
        
        # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ìë¥´ê¸°
        if len(content) > 1000:
            content = content[:1000] + "..."
        
        return content

    def create_enhanced_risk_assessment_prompt(self) -> ChatPromptTemplate:
        """
        ì´ìƒì¹˜ ì •ë³´ë¥¼ ë°˜ì˜í•œ í–¥ìƒëœ ì‹ ìš©ìœ„í—˜ íŒë‹¨ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Returns:
            ChatPromptTemplate: í–¥ìƒëœ ìœ„í—˜ í‰ê°€ í”„ë¡¬í”„íŠ¸
        """
        system_message = """
ë‹¹ì‹ ì€ ì‹ ìš©ìœ„í—˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì—…ì˜ ì¬ë¬´/ë¹„ì¬ë¬´ ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ê³ ë ¤í•˜ì—¬ ë‰´ìŠ¤ê°€ ì‹ ìš©ìœ„í—˜ ì§•í›„ì¸ì§€ íŒë‹¨í•˜ê³ , êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

**ì¤‘ìš”í•œ íŒë‹¨ ì›ì¹™**:
1. **ì„ ë³„ëœ ë‰´ìŠ¤ ë¶„ì„**: ì…ë ¥ëœ ë‰´ìŠ¤ëŠ” ì´ë¯¸ ì´ìƒ ì§•í›„ ë° ì‹ ìš©ìœ„í—˜ ê¸°ì¤€ì— ë”°ë¼ 1ì°¨ ì„ ë³„ëœ ê²ƒì…ë‹ˆë‹¤. ì´ ë‰´ìŠ¤ë“¤ì´ ì™œ, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜ ìœ„í—˜í•œì§€ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
2. **ëª…í™•í•œ ê·¼ê±° ê¸°ë°˜ íŒë‹¨**: ê° ë‰´ìŠ¤ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì„¤ëª… ê°€ëŠ¥í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
3. **ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„± ì¤‘ì‹œ**: íƒì§€ëœ ì¬ë¬´/ë¹„ì¬ë¬´ ì´ìƒì¹˜ì™€ ë‰´ìŠ¤ì˜ ì—°ê´€ì„±ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
4. **ë³µí•©ì  ìœ„í—˜ í‰ê°€**: ë‰´ìŠ¤ ë‹¨ë…ìœ¼ë¡œëŠ” ì •ìƒì´ì§€ë§Œ ì´ìƒì¹˜ì™€ í•¨ê»˜ ë³´ë©´ ìœ„í—˜í•œ ê²½ìš°ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
5. **ì •ëŸ‰ì  ìœ„í—˜ë„ í‰ê°€**: ìœ„í—˜ë„ë¥¼ 1-10ì ìœ¼ë¡œ ìˆ˜ì¹˜í™”í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”.

**ë³µí•© ìœ„í—˜ íŒë‹¨ ì˜ˆì‹œ**:
- ë‰´ìŠ¤: "ê¸°ì—…ì´ ìƒˆë¡œìš´ ì‚¬ì—…ì„ ì‹œì‘"
- ì¬ë¬´ ì´ìƒì¹˜: "ë¶€ì±„ë¹„ìœ¨ì´ ë†’ìŒ"
- íŒë‹¨: ë¶€ì±„ë¹„ìœ¨ì´ ë†’ì€ ìƒí™©ì—ì„œ ì‹ ê·œ ì‚¬ì—… íˆ¬ìëŠ” ì¬ë¬´ ë¶€ë‹´ ê°€ì¤‘ìœ¼ë¡œ ìœ„í—˜ (ìœ„í—˜ë„ 7ì )

**íƒì§€ëœ ì´ìƒì¹˜**:
ì¬ë¬´ ì´ìƒì¹˜: {financial_anomalies_summary}
ë¹„ì¬ë¬´ ì´ìƒì¹˜: {non_financial_anomalies_summary}
í˜„ì¬ ì‹ ìš©ë“±ê¸‰: {current_credit_grade}

**ì‹ ìš©ìœ„í—˜ ì§•í›„ ê¸°ì¤€**:
{credit_risk_criteria}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "analysis_date": {analysis_date},
    "company_name": "ê¸°ì—…ëª…",
    "total_risk_news": ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
    "high_risk_count": ê³ ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
    "medium_risk_count": ì¤‘ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
    "key_risk_summary": "ì£¼ìš” ìœ„í—˜ ìš”ì•½ (2-3ë¬¸ì¥)",
    "risk_news_items": [
        {{
            "title": "ë‰´ìŠ¤ ì œëª©",
            "content_summary": "ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½ (1-2ë¬¸ì¥)",
            "url": "ë‰´ìŠ¤ URL",
            "risk_category": "ì¬ë¬´ìœ„í—˜/ë¹„ì¬ë¬´ìœ„í—˜/ë³µí•©ìœ„í—˜",
            "risk_level": ìœ„í—˜ë„(1-10),
            "evidence_basis": "ìœ„í—˜ íŒë‹¨ ê·¼ê±° (êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ)",
            "anomaly_connection": "íƒì§€ëœ ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„±",
            "published_date": "ë°œí–‰ì¼"
        }}
    ],
    "credit_assessment": {{
        "current_grade": "í˜„ì¬ ì¶”ì • ì‹ ìš©ë“±ê¸‰",
        "predicted_grade": "ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰",
        "change_probability": ë³€ê²½í™•ë¥ (0-1),
        "change_direction": "ìƒìŠ¹/í•˜ë½/ìœ ì§€",
        "reasoning": "ë“±ê¸‰ í‰ê°€ ê·¼ê±°"
    }}
}}

**ìœ„í—˜ë„ í‰ê°€ ê¸°ì¤€**:
- 9-10ì : ì‹¬ê°í•œ ìœ„í—˜ (ì‹ ìš©ë“±ê¸‰ ì¦‰ì‹œ í•˜ë½ ìš”ì¸)
- 7-8ì : ë†’ì€ ìœ„í—˜ (ë‹¨ê¸°ë‚´ ì‹ ìš©ë“±ê¸‰ì— ë¶€ì •ì  ì˜í–¥)
- 5-6ì : ì¤‘ê°„ ìœ„í—˜ (ì§€ì† ëª¨ë‹ˆí„°ë§ í•„ìš”)
- 3-4ì : ë‚®ì€ ìœ„í—˜ (ì ì¬ì  ìš°ë ¤ì‚¬í•­)
- 1-2ì : ë¯¸ë¯¸í•œ ìœ„í—˜ (ì¼ë°˜ì  ë¹„ì¦ˆë‹ˆìŠ¤ í™œë™)

**ì¤‘ìš”**: ìœ„í—˜ë„ 5ì  ì´ìƒì¸ ë‰´ìŠ¤ë§Œ risk_news_itemsì— í¬í•¨í•˜ì„¸ìš”. ìœ„í—˜ ë‰´ìŠ¤ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° risk_news_itemsë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
"""
        
        human_message = """
## íƒì§€ëœ ì´ìƒì¹˜ ì •ë³´

### ì¬ë¬´ ì´ìƒì¹˜
{financial_anomalies_detail}

### ë¹„ì¬ë¬´ ì´ìƒì¹˜  
{non_financial_anomalies_detail}

### ê¸°ì—… ì¬ë¬´ íŠ¹ì„±
{financial_characteristics}

## ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤ (1ì°¨ ì„ ë³„ë¨)
{news_data}

ìœ„ ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ìš©ìœ„í—˜ ì§•í›„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  êµ¬ì¡°í™”ëœ JSON ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        return ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("human", human_message)
        ])
    
    def assess_credit_risk_with_anomalies(self, analysis_data: Dict[str, Any], news_list: List[Dict[str, Any]]) -> DailyRiskSummary:
        """
        ì´ìƒì¹˜ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë‰´ìŠ¤ì˜ ì‹ ìš©ìœ„í—˜ ì§•í›„ ì—¬ë¶€ íŒë‹¨
        
        Args:
            analysis_data (Dict): ì¬ë¬´/ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼
            news_list (List[Dict]): **ì‚¬ì „ í•„í„°ë§ëœ** ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            DailyRiskSummary: êµ¬ì¡°í™”ëœ ì¼ì¼ ìœ„í—˜ ë¶„ì„ ê²°ê³¼
        """
        if not news_list:
            # ë‰´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
            return DailyRiskSummary(
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                company_name=analysis_data.get("company_name", "Unknown"),
                total_risk_news=0,
                high_risk_count=0,
                medium_risk_count=0,
                key_risk_summary="ì˜¤ëŠ˜ ê´€ë ¨ëœ ìœ„í—˜ ë‰´ìŠ¤ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì‚¬ì „ í•„í„°ë§ ê²°ê³¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ)",
                risk_news_items=[],
                credit_assessment=CreditAssessment(
                    current_grade=analysis_data.get("current_credit_grade", "A"),
                    predicted_grade=analysis_data.get("current_credit_grade", "A"),
                    change_probability=0.0,
                    change_direction="ìœ ì§€",
                    reasoning="ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ì–´ í˜„ì¬ ë“±ê¸‰ ìœ ì§€ ì˜ˆìƒ"
                )
            )
        
        try:
            prompt = self.create_enhanced_risk_assessment_prompt()
            
            # JSON ì¶œë ¥ íŒŒì„œ ì„¤ì •
            json_parser = JsonOutputParser(pydantic_object=DailyRiskSummary)
            chain = prompt | self.llm | json_parser
            
            # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸¸ì´ ì œí•œ)
            news_text = "\n\n".join([
                f"[ë‰´ìŠ¤ {i+1}]\nì œëª©: {news.get('title', '')}\në‚´ìš©: {news.get('content', '')[:500]}...\nURL: {news.get('url', 'URL ì—†ìŒ')}\në°œí–‰ì¼: {news.get('published_date', '')}"
                for i, news in enumerate(news_list[:8])  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
            ])
            
            # ì´ìƒì¹˜ ìƒì„¸ ì •ë³´ ìƒì„±
            financial_detail = "\n".join([
                f"- {anomaly.get('indicator', '')}: {anomaly.get('description', '')} (ì‹¬ê°ë„: {anomaly.get('severity', '')})"
                for anomaly in analysis_data.get("financial_anomalies", [])
            ]) or "íƒì§€ëœ ì¬ë¬´ ì´ìƒì¹˜ ì—†ìŒ"
            
            nfr_detail = "\n".join([
                f"- {anomaly.get('indicator', '')} ({anomaly.get('pillar', '')}): {anomaly.get('description', '')} (ì‹¬ê°ë„: {anomaly.get('severity', '')})"
                for anomaly in analysis_data.get("non_financial_anomalies", [])[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            ]) or "íƒì§€ëœ ë¹„ì¬ë¬´ ì´ìƒì¹˜ ì—†ìŒ"
            
            # ì´ìƒì¹˜ ìš”ì•½ ìƒì„±
            financial_summary = f"{len(analysis_data.get('financial_anomalies', []))}ê°œ ì¬ë¬´ ì´ìƒì¹˜"
            nfr_summary = f"{len(analysis_data.get('non_financial_anomalies', []))}ê°œ ë¹„ì¬ë¬´ ì´ìƒì¹˜"
            
            result = chain.invoke({
                "analysis_date" : datetime.now().strftime("%Y-%m-%d"),
                "financial_anomalies_summary": financial_summary,
                "non_financial_anomalies_summary": nfr_summary,
                "current_credit_grade": analysis_data.get("current_credit_grade", "A"),
                "credit_risk_criteria": json.dumps(self.credit_risk_criteria, ensure_ascii=False, indent=1),
                "financial_anomalies_detail": financial_detail,
                "non_financial_anomalies_detail": nfr_detail,
                "financial_characteristics": json.dumps(analysis_data.get("financial_characteristics", {}), ensure_ascii=False),
                "news_data": news_text
            })
            
            # Pydantic ëª¨ë¸ë¡œ ê²°ê³¼ ê²€ì¦ ë° ë³€í™˜
            if isinstance(result, dict):
                return DailyRiskSummary(**result)
            else:
                return result
            
        except Exception as e:
            print(f"âš ï¸ LLM API ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            return DailyRiskSummary(
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                company_name=analysis_data.get("company_name", "Unknown"),
                total_risk_news=0,
                high_risk_count=0,
                medium_risk_count=0,
                key_risk_summary=f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                risk_news_items=[],
                credit_assessment=CreditAssessment(
                    current_grade=analysis_data.get("current_credit_grade", "A"),
                    predicted_grade=analysis_data.get("current_credit_grade", "A"),
                    change_probability=0.0,
                    change_direction="ìœ ì§€",
                    reasoning="ë¶„ì„ ì˜¤ë¥˜ë¡œ ì¸í•´ ë“±ê¸‰ ë³€ê²½ ì˜ˆì¸¡ ë¶ˆê°€"
                )
            )
    
    def generate_daily_summary_report(self, risk_summary: DailyRiskSummary) -> str:
        """
        ì¼ì¼ ì‹ ìš©ìœ„í—˜ ì•Œë¦¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            risk_summary: ì¼ì¼ ìœ„í—˜ ë¶„ì„ ê²°ê³¼
            
        Returns:
            str: ì•Œë¦¼ í˜•ì‹ì˜ ìš”ì•½ ë¦¬í¬íŠ¸
        """
        
        # ì•Œë¦¼ í—¤ë”
        report = f"""ğŸš¨ {risk_summary.company_name} ì‹ ìš©ìœ„í—˜ ì•Œë¦¼ ìš”ì•½

    ğŸ“… ë¶„ì„ì¼ì: {risk_summary.analysis_date}
    ğŸ“Š ìœ„í—˜ ë‰´ìŠ¤: {risk_summary.total_risk_news}ê°œ (ê³ ìœ„í—˜ {risk_summary.high_risk_count}ê°œ, ì¤‘ìœ„í—˜ {risk_summary.medium_risk_count}ê°œ)
    âš–ï¸ ì‹ ìš©ë“±ê¸‰: {risk_summary.credit_assessment.current_grade} â†’ {risk_summary.credit_assessment.predicted_grade} ({risk_summary.credit_assessment.change_probability:.1%})

    """
    
        if risk_summary.risk_news_items:
            # ìœ„í—˜ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_news = sorted(risk_summary.risk_news_items, key=lambda x: x.risk_level, reverse=True)
            
            for news in sorted_news:
                # ìœ„í—˜ë„ì— ë”°ë¥¸ ì´ëª¨ì§€ ì„¤ì •
                if news.risk_level >= 8:
                    risk_emoji = "ğŸ”¥"
                    alert_level = "ê¸´ê¸‰"
                elif news.risk_level >= 6:
                    risk_emoji = "âš ï¸"
                    alert_level = "ì£¼ì˜"
                else:
                    risk_emoji = "â„¹ï¸"
                    alert_level = "ëª¨ë‹ˆí„°ë§"
                    
                report += f"""
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    {risk_emoji} {news.risk_category} ë¦¬ìŠ¤í¬ {alert_level} ê²½ê³ 
    [{news.published_date}] "{news.title}"

    """
                
                # ì´ìƒì¹˜ ì—°ê´€ì„±ì´ ìˆëŠ” ê²½ìš° íŠ¹ë³„ ê°•ì¡°
                if news.anomaly_connection and news.anomaly_connection.strip() != "í•´ë‹¹ì—†ìŒ":
                    report += f"""ğŸ” ì´ìƒì§•í›„ íƒì§€ ê·¼ê±°:
    {news.anomaly_connection}

    ğŸ“ˆ ë‰´ìŠ¤ ë¶„ì„:
    {news.content_summary}

    ğŸ¤– AI ìœ„í—˜ë„ í‰ê°€:
    {news.evidence_basis}
    (ìœ„í—˜ë„: {news.risk_level}/10)

    """
                else:
                    report += f"""ğŸ“ˆ ìƒí™© ë¶„ì„:
    {news.content_summary}

    ğŸ¤– AI ìœ„í—˜ë„ í‰ê°€:
    {news.evidence_basis}
    (ìœ„í—˜ë„: {news.risk_level}/10)

    """
        else:
            report += """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    âœ… ì˜¤ëŠ˜ íƒì§€ëœ ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.

    """
        
        # ì¢…í•© í‰ê°€
        report += f"""
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ğŸ“‹ ì¢…í•© ìœ„í—˜ë„ í‰ê°€

    {risk_summary.key_risk_summary}

    ğŸ’¡ ì‹ ìš©ë“±ê¸‰ ë³€ë™ ì „ë§:
    í˜„ì¬ {risk_summary.credit_assessment.current_grade}ë“±ê¸‰ì—ì„œ {risk_summary.credit_assessment.predicted_grade}ë“±ê¸‰ìœ¼ë¡œ {risk_summary.credit_assessment.change_direction}í•  í™•ë¥ ì´ {risk_summary.credit_assessment.change_probability:.1%}ì…ë‹ˆë‹¤.

    ê·¼ê±°: {risk_summary.credit_assessment.reasoning}

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â° ì•Œë¦¼ ìƒì„±: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    ğŸ”„ ë‹¤ìŒ ëª¨ë‹ˆí„°ë§: {(datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")}
    """
        
        return report
    
    def analyze_credit_risk_with_results(self, result_dir: str) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ìš©ìœ„í—˜ ì§•í›„ ì¢…í•© ë¶„ì„
        
        Args:
            result_dir (str): ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            Dict: ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        print("=== ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘ ===")
        
        # 1. ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        print("1. ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        analysis_data = self.load_analysis_results(result_dir)
        
        company_name = analysis_data.get("company_name", "Unknown")
        financial_anomalies_count = len(analysis_data.get("financial_anomalies", []))
        nfr_anomalies_count = len(analysis_data.get("non_financial_anomalies", []))
        
        print(f"   - ê¸°ì—…ëª…: {company_name}")
        print(f"   - ì¬ë¬´ ì´ìƒì¹˜: {financial_anomalies_count}ê°œ")
        print(f"   - ë¹„ì¬ë¬´ ë¶„ì„ í•­ëª©: {nfr_anomalies_count}ê°œ")
        
        # 2. íƒ€ê²ŸíŒ…ëœ ë‰´ìŠ¤ ê²€ìƒ‰
        print("\n2. ì´ìƒì¹˜ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        targeted_news = self.search_targeted_news(analysis_data)
        print(f"   - ê²€ìƒ‰ëœ ì´ ë‰´ìŠ¤ ìˆ˜: {len(targeted_news)}ê°œ")
        
        # 2.5. ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§ (í•µì‹¬ ë³€ê²½ì‚¬í•­)
        print("\n2.5. ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§ ì¤‘...")
        relevant_news = self._pre_filter_relevant_news(targeted_news, analysis_data)

        # 3. ì‹ ìš©ìœ„í—˜ ì§•í›„ ì‹¬ì¸µ ë¶„ì„
        print("\n3. ì‹ ìš©ìœ„í—˜ ì§•í›„ ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        risk_summary = self.assess_credit_risk_with_anomalies(analysis_data, relevant_news)
        
        # 4. ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        print("\n4. ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        daily_report = self.generate_daily_summary_report(risk_summary)
        
        # 5. ê²°ê³¼ ì •ë¦¬
        analysis_result = {
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "company": company_name,
            "total_news_count": len(targeted_news),
            "relevant_news_count": len(relevant_news),
            "financial_anomalies_count": financial_anomalies_count,
            "non_financial_anomalies_count": nfr_anomalies_count,
            "company_analysis_summary": self._create_analysis_summary(analysis_data),
            "relevant_news": relevant_news, # í•„í„°ë§ëœ ë‰´ìŠ¤
            "daily_risk_summary": risk_summary.dict(),
            "daily_summary_report": daily_report,
            "analysis_basis": "detected_anomalies_and_pre_filtered_news"
        }
        
        # 6. ê²°ê³¼ ì €ì¥
        try:
            # JSON ê²°ê³¼ ì €ì¥
            json_output_file = os.path.join(result_dir, "daily_news_risk_analysis.json")
            with open(json_output_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… JSON ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_output_file}")
            
            # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥
            report_output_file = os.path.join(result_dir, "daily_risk_summary.md")
            with open(report_output_file, 'w', encoding='utf-8') as f:
                f.write(daily_report)
            print(f"âœ… ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_output_file}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        
        print("\n=== ë¶„ì„ ì™„ë£Œ ===")
        return analysis_result
    
    def _create_analysis_summary(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ì •ë³´ ìƒì„±"""
        
        financial_anomalies = analysis_data.get("financial_anomalies", [])
        nfr_anomalies = analysis_data.get("non_financial_anomalies", [])
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê³„ì‚° (ì¬ë¬´)
        financial_high_risk = sum(1 for a in financial_anomalies if a.get("severity") in ["high", "critical"])
        financial_medium_risk = sum(1 for a in financial_anomalies if a.get("severity") == "medium")
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê³„ì‚° (ë¹„ì¬ë¬´)
        nfr_high_risk = sum(1 for a in nfr_anomalies if a.get("severity") in ["high"])
        nfr_medium_risk = sum(1 for a in nfr_anomalies if a.get("severity") == "medium")
        
        total_high_risk = financial_high_risk + nfr_high_risk
        total_anomalies = len(financial_anomalies) + len(nfr_anomalies)
        
        # ì „ì²´ ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
        if total_high_risk >= 3 or total_anomalies >= 10:
            overall_risk = "ë†’ìŒ"
        elif total_high_risk >= 1 or total_anomalies >= 6:
            overall_risk = "ì£¼ì˜"
        elif total_anomalies >= 3:
            overall_risk = "ë³´í†µ"
        else:
            overall_risk = "ë‚®ìŒ"
        
        # ì£¼ìš” ìš°ë ¤ì‚¬í•­ ì¶”ì¶œ
        concerns = []
        
        # ì¬ë¬´ ìš°ë ¤ì‚¬í•­
        for anomaly in financial_anomalies:
            if anomaly.get("severity") in ["high", "medium", "critical"]:
                concerns.append(f"ì¬ë¬´: {anomaly.get('description', '')}")
        
        # ë¹„ì¬ë¬´ ìš°ë ¤ì‚¬í•­
        for anomaly in nfr_anomalies:
            if anomaly.get("severity") in ["high", "medium"]:
                concerns.append(f"ë¹„ì¬ë¬´({anomaly.get('pillar', '')}): {anomaly.get('description', '')}")
        
        return {
            "overall_risk_level": overall_risk,
            "total_anomalies": total_anomalies,
            "high_risk_anomalies": total_high_risk,
            "financial_risk_count": len(financial_anomalies),
            "non_financial_risk_count": len(nfr_anomalies),
            "financial_characteristics": analysis_data.get("financial_characteristics", {}),
            "key_concerns": concerns[:5]  # ìƒìœ„ 5ê°œ
        }


# ì‚¬ìš© ì˜ˆì œ
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì´ìƒì¹˜ ê¸°ë°˜ ë¶„ì„)
    """
    try:
        # API í‚¤ ì„¤ì •
        from dotenv import load_dotenv
        load_dotenv()
        
        import os
        if not os.getenv("OPENAI_API_KEY"):
            print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
            
        if not os.getenv("TAVILY_API_KEY"):
            print("âš ï¸ TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("=== ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ===")
        analyzer = CreditRiskNewsAnalyzer(max_search_results=5)
        
        # ì˜ˆì‹œ ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” run.pyì—ì„œ ì „ë‹¬)
        test_result_dir = "analysis_results/ì‚¼ì„±ì „ì"
        if not os.path.exists(test_result_dir):
            os.makedirs(test_result_dir)
            print(f"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±: {test_result_dir}")
        
        # ì´ìƒì¹˜ ê¸°ë°˜ ì‹ ìš©ìœ„í—˜ ì§•í›„ ë¶„ì„ ì‹¤í–‰
        print("\n=== ì´ìƒì¹˜ ê¸°ë°˜ ì‹ ìš©ìœ„í—˜ ì§•í›„ ë¶„ì„ ì‹¤í–‰ ===")
        analysis_result = analyzer.analyze_credit_risk_with_results(test_result_dir)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“° ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ")
        print("="*60)
        
        risk_summary = analysis_result.get("daily_risk_summary", {})
        
        print(f"ğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"- ë¶„ì„ ëŒ€ìƒ: {analysis_result['company']}")
        print(f"- ì´ ê²€ìƒ‰ëœ ë‰´ìŠ¤: {analysis_result['total_news_count']}ê°œ")
        print(f"- ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤: {analysis_result['relevant_news_count']}ê°œ (ì„ ë³„ë¨)")
        print(f"- ìµœì¢… ë¶„ì„ëœ ìœ„í—˜ ë‰´ìŠ¤: {risk_summary.get('total_risk_news', 0)}ê°œ")
        print(f"  - ê³ ìœ„í—˜: {risk_summary.get('high_risk_count', 0)}ê°œ, ì¤‘ìœ„í—˜: {risk_summary.get('medium_risk_count', 0)}ê°œ")
        
        credit_assessment = risk_summary.get('credit_assessment', {})
        print(f"\nğŸ“ˆ ì‹ ìš©ë“±ê¸‰ í‰ê°€:")
        print(f"- í˜„ì¬ ì‹ ìš©ë“±ê¸‰: {credit_assessment.get('current_grade', 'Unknown')}")
        print(f"- ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰: {credit_assessment.get('predicted_grade', 'Unknown')}")
        print(f"- ë³€ê²½ í™•ë¥ : {credit_assessment.get('change_probability', 0):.1%}")
        print(f"- ë³€ê²½ ë°©í–¥: {credit_assessment.get('change_direction', 'Unknown')}")
        
        print(f"\nğŸ“„ ìƒì„±ëœ íŒŒì¼:")
        print(f"- {os.path.join(test_result_dir, 'daily_news_risk_analysis.json')}")
        print(f"- {os.path.join(test_result_dir, 'daily_risk_summary.md')}")
                
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()