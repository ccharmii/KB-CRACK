# /KB-CRACK/news_analysis/news_search.py
# ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ì´ìƒì¹˜ ì—°ê³„ ìœ„í—˜ë„ í‰ê°€ ë¡œì§

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field


class NewsRiskItem(BaseModel):
    """ë‰´ìŠ¤ ìœ„í—˜ í•­ëª© ë°ì´í„° ëª¨ë¸"""

    title: str = Field(description="ë‰´ìŠ¤ ì œëª©")
    content_summary: str = Field(description="ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½ (1-2ë¬¸ì¥)")
    url: str = Field(description="ë‰´ìŠ¤ URL")
    risk_category: str = Field(description="ìœ„í—˜ ì¹´í…Œê³ ë¦¬ (ì¬ë¬´ìœ„í—˜/ë¹„ì¬ë¬´ìœ„í—˜/ë³µí•©ìœ„í—˜)")
    risk_level: int = Field(description="ìœ„í—˜ë„ (1-10, 10ì´ ê°€ì¥ ìœ„í—˜)")
    evidence_basis: str = Field(description="ìœ„í—˜ íŒë‹¨ ê·¼ê±°")
    anomaly_connection: str = Field(description="íƒì§€ëœ ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„±")
    published_date: str = Field(description="ë°œí–‰ì¼", default="")


class CreditAssessment(BaseModel):
    """ì‹ ìš©ë“±ê¸‰ í‰ê°€ ë°ì´í„° ëª¨ë¸"""

    current_grade: str = Field(description="í˜„ì¬ ì¶”ì • ì‹ ìš©ë“±ê¸‰")
    predicted_grade: str = Field(description="ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰")
    change_probability: float = Field(description="ë“±ê¸‰ ë³€ê²½ í™•ë¥  (0-1)")
    change_direction: str = Field(description="ë³€ê²½ ë°©í–¥ (ìƒìŠ¹/í•˜ë½/ìœ ì§€)")
    reasoning: str = Field(description="ë“±ê¸‰ í‰ê°€ ê·¼ê±°")


class DailyRiskSummary(BaseModel):
    """ì¼ì¼ ìœ„í—˜ ìš”ì•½ ë°ì´í„° ëª¨ë¸"""

    analysis_date: str = Field(description="ë¶„ì„ ë‚ ì§œ")
    company_name: str = Field(description="ê¸°ì—…ëª…")
    total_risk_news: int = Field(description="ìœ„í—˜ ë‰´ìŠ¤ ì´ ê°œìˆ˜")
    high_risk_count: int = Field(description="ê³ ìœ„í—˜ ë‰´ìŠ¤ ê°œìˆ˜ (ìœ„í—˜ë„ 7-10)")
    medium_risk_count: int = Field(description="ì¤‘ìœ„í—˜ ë‰´ìŠ¤ ê°œìˆ˜ (ìœ„í—˜ë„ 4-6)")
    key_risk_summary: str = Field(description="ì£¼ìš” ìœ„í—˜ ìš”ì•½ (2-3ë¬¸ì¥)")
    risk_news_items: List[NewsRiskItem] = Field(description="ìœ„í—˜ ë‰´ìŠ¤ ëª©ë¡")
    credit_assessment: CreditAssessment = Field(description="ì‹ ìš©ë“±ê¸‰ í‰ê°€")


class NewsRelevance(BaseModel):
    """ë‰´ìŠ¤ ê´€ë ¨ì„± íŒë³„ ë°ì´í„° ëª¨ë¸"""

    is_relevant: bool = Field(description="ë‰´ìŠ¤ê°€ ì‹ ìš©ìœ„í—˜ ë¶„ì„ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€")
    reason: str = Field(description="ê´€ë ¨ì„± íŒë‹¨ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±°")


class CreditRiskNewsAnalyzer:
    def __init__(self, max_search_results: int = 10, openai_api_key: str | None = None, tavily_api_key: str | None = None):
        """
        ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Args:
            max_search_results: ë‰´ìŠ¤ ê²€ìƒ‰ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            openai_api_key: OpenAI API í‚¤
            tavily_api_key: Tavily API í‚¤
        """
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        if tavily_api_key:
            os.environ["TAVILY_API_KEY"] = tavily_api_key

        self.tavily_search = TavilySearch(max_results=max_search_results)
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)

        self.credit_risk_criteria = {
            "ì¬ë¬´ ìœ„í—˜ ì§•í›„": {
                "ìˆ˜ìµì„± ì•…í™”": ["ë§¤ì¶œ ê°ì†Œ", "ì´ìµë¥  í•˜ë½", "ì†ì‹¤ í™•ëŒ€", "ì˜ì—…ì ì"],
                "ì¬ë¬´êµ¬ì¡° ì•…í™”": ["ë¶€ì±„ ì¦ê°€", "ì°¨ì…ê¸ˆ ì¦ê°€", "ì‹ ìš©ë“±ê¸‰ í•˜ë½", "ê¸ˆìœµë¹„ìš© ì¦ê°€"],
                "ìœ ë™ì„± ìœ„ê¸°": ["í˜„ê¸ˆ ë¶€ì¡±", "ìœ ë™ì„± ê²½ìƒ‰", "ìê¸ˆì¡°ë‹¬ ì–´ë ¤ì›€", "ìš´ì „ìê¸ˆ ë¶€ì¡±"],
                "íˆ¬ì ëŠ¥ë ¥ ì €í•˜": ["íˆ¬ì ì¶•ì†Œ", "ì„¤ë¹„íˆ¬ì ê°ì†Œ", "ì—°êµ¬ê°œë°œë¹„ ì‚­ê°", "ì‹ ê·œíˆ¬ì ì¤‘ë‹¨"],
            },
            "ë¹„ì¬ë¬´ ìœ„í—˜ ì§•í›„": {
                "ê±°ë²„ë„ŒìŠ¤ ì´ìŠˆ": ["ì§€ë°°êµ¬ì¡° ë¬¸ì œ", "ê²½ì˜ì§„ êµì²´", "ë‚´ë¶€ ê°ˆë“±", "ì£¼ì£¼ ë¶„ìŸ"],
                "ê·œì œ/ë²•ì  ë¦¬ìŠ¤í¬": ["ê·œì œ ìœ„ë°˜", "ê³¼ì§•ê¸ˆ", "ì†Œì†¡", "ë²•ì  ë¶„ìŸ", "ì œì¬ ì¡°ì¹˜"],
                "ìš´ì˜ ë¦¬ìŠ¤í¬": ["ìƒì‚° ì¤‘ë‹¨", "ê³µê¸‰ë§ ì°¨ì§ˆ", "í’ˆì§ˆ ë¬¸ì œ", "ì‚¬ê³  ë°œìƒ"],
                "ESG ë¦¬ìŠ¤í¬": ["í™˜ê²½ ì˜¤ì—¼", "ì‚¬íšŒì  ë¬¼ì˜", "ë…¸ì‚¬ ê°ˆë“±", "í‰íŒ ì•…í™”"],
                "ì‹œì¥ ë¦¬ìŠ¤í¬": ["ì‹œì¥ì ìœ ìœ¨ í•˜ë½", "ê²½ìŸ ì‹¬í™”", "ê³ ê° ì´íƒˆ", "ë¸Œëœë“œ ê°€ì¹˜ í•˜ë½"],
            },
        }

    def load_analysis_results(self, result_dir: str) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ë¬´ë¶„ì„ê³¼ ë¹„ì¬ë¬´ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        Args:
            result_dir: ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        Returns:
            í†µí•©ëœ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        """
        analysis_data: Dict[str, Any] = {
            "company_name": "",
            "current_credit_grade": "A",
            "financial_anomalies": [],
            "non_financial_anomalies": [],
            "financial_characteristics": {},
            "business_context": {},
        }

        financial_anomalies_file = os.path.join(result_dir, "financial_anomalies.json")
        if os.path.exists(financial_anomalies_file):
            try:
                with open(financial_anomalies_file, "r", encoding="utf-8") as f:
                    raw_financial_anomalies = json.load(f)

                financial_anomalies = []
                if isinstance(raw_financial_anomalies, dict):
                    for metric_name, details in raw_financial_anomalies.items():
                        if isinstance(details, dict):
                            financial_anomalies.append(
                                {
                                    "indicator": details.get("metric_name", metric_name),
                                    "severity": details.get("severity", "medium").lower(),
                                    "description": details.get("description", ""),
                                    "type": details.get("type", ""),
                                    "quarter": details.get("quarter", ""),
                                    "source": details.get("source", ""),
                                    "metric_name": details.get("metric_name", metric_name),
                                }
                            )
                        else:
                            financial_anomalies.append(
                                {
                                    "indicator": metric_name,
                                    "severity": "medium",
                                    "description": str(details),
                                    "type": "unknown",
                                    "quarter": "Latest",
                                    "source": "unknown",
                                    "metric_name": metric_name,
                                }
                            )
                elif isinstance(raw_financial_anomalies, list):
                    for anomaly in raw_financial_anomalies:
                        financial_anomalies.append(
                            {
                                "indicator": anomaly.get("metric_name", ""),
                                "severity": anomaly.get("severity", "medium").lower(),
                                "description": anomaly.get("description", ""),
                                "type": anomaly.get("type", ""),
                                "quarter": anomaly.get("quarter", ""),
                                "source": anomaly.get("source", ""),
                            }
                        )

                analysis_data["financial_anomalies"] = financial_anomalies
                print(f"âœ… ì¬ë¬´ ì´ìƒì¹˜ ë¡œë“œ ì™„ë£Œ: {len(financial_anomalies)}ê°œ")

            except Exception as e:
                print(f"âŒ ì¬ë¬´ ì´ìƒì¹˜ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

        financial_analysis_file = os.path.join(result_dir, "financial_analysis.json")
        if os.path.exists(financial_analysis_file):
            try:
                with open(financial_analysis_file, "r", encoding="utf-8") as f:
                    financial_data = json.load(f)

                company_info = financial_data.get("ê¸°ì—…_ì •ë³´", {})
                analysis_data["company_name"] = company_info.get("ê¸°ì—…ëª…", "Unknown")
                analysis_data["current_credit_grade"] = company_info.get("Current_credit_grade", "A")

                analysis_data["business_context"] = {
                    "company_info": company_info,
                    "industry": company_info.get("ì—…ì¢…", ""),
                    "business_area": company_info.get("ì œí’ˆêµ°", ""),
                }

                print(f"âœ… ê¸°ì—… ì •ë³´ ë¡œë“œ ì™„ë£Œ: {analysis_data['company_name']}")

            except Exception as e:
                print(f"âŒ ê¸°ì—… ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
                analysis_data["company_name"] = "ì‚¼ì„±ì „ì"

        nfr_file = os.path.join(result_dir, "non_financial_reasoning.json")
        if os.path.exists(nfr_file):
            try:
                with open(nfr_file, "r", encoding="utf-8") as f:
                    nfr_data = json.load(f)

                if nfr_data.get("success") and "results" in nfr_data:
                    quarter = nfr_data.get("quarter", "")

                    for result in nfr_data["results"]:
                        metric = result.get("metric", "")
                        anomaly_text = result.get("anomaly_text", "")
                        explanation = result.get("explanation_ko", "")
                        confidence = result.get("confidence", 0)

                        severity_text = result.get("severity", "Medium")
                        severity = severity_text.lower() if severity_text in ["High", "Medium", "Low"] else "medium"

                        analysis_data["non_financial_anomalies"].append(
                            {
                                "indicator": metric,
                                "pillar": "Unknown",
                                "score": 0,
                                "grade_label": anomaly_text,
                                "severity": severity,
                                "description": explanation,
                                "confidence": confidence,
                                "quarter": quarter,
                                "indicator_id": metric,
                            }
                        )

                    print(f"âœ… ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(analysis_data['non_financial_anomalies'])}ê°œ í•­ëª©")

                else:
                    print(f"âŒ ë¹„ì¬ë¬´ë¶„ì„ JSON êµ¬ì¡° ë¶ˆì¼ì¹˜: success={nfr_data.get('success')}")

            except Exception as e:
                print(f"âŒ ë¹„ì¬ë¬´ë¶„ì„ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

        analysis_data["financial_characteristics"] = self._analyze_financial_characteristics(analysis_data["financial_anomalies"])

        return analysis_data

    def _analyze_financial_characteristics(self, anomalies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¬ë¬´ ì´ìƒì¹˜ ëª©ë¡ì—ì„œ ì‹¬ê°ë„ ë° ìœ í˜• ë¶„í¬ íŠ¹ì„± ì¶”ì¶œ"""

        characteristics: Dict[str, Any] = {
            "severity_distribution": {"high": 0, "medium": 0, "low": 0},
            "type_distribution": {"peer_comparison": 0, "time_series": 0, "other": 0},
            "indicator_types": {},
            "quarters_affected": set(),
            "risk_indicators": [],
            "total_anomalies": len(anomalies),
        }

        for anomaly in anomalies:
            severity = anomaly.get("severity", "medium").lower()
            if severity in characteristics["severity_distribution"]:
                characteristics["severity_distribution"][severity] += 1

            anomaly_type = anomaly.get("type", "other")
            if anomaly_type in characteristics["type_distribution"]:
                characteristics["type_distribution"][anomaly_type] += 1
            else:
                characteristics["type_distribution"]["other"] += 1

            indicator = anomaly.get("indicator", "")
            if indicator:
                characteristics["indicator_types"][indicator] = characteristics["indicator_types"].get(indicator, 0) + 1

            quarter = anomaly.get("quarter", "")
            if quarter:
                characteristics["quarters_affected"].add(quarter)

            if severity == "high":
                characteristics["risk_indicators"].append(indicator)

        characteristics["quarters_affected"] = list(characteristics["quarters_affected"])

        return characteristics

    def _generate_augmented_queries(self, company_name: str) -> List[str]:
        """íšŒì‚¬ëª…ê³¼ ìœ„í—˜ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""

        base_query = f'"{company_name}"'

        financial_risk_keywords = [
            "ìœ ë™ì„± ìœ„ê¸°",
            "ìê¸ˆë‚œ",
            "ë¶€ì±„ ê³¼ë‹¤",
            "ì˜ì—…ì†ì‹¤",
            "ì ì ì§€ì†",
            "ì‹ ìš©ë“±ê¸‰ í•˜í–¥",
            "ì±„ë¬´ ë¶ˆì´í–‰",
            "ì›Œí¬ì•„ì›ƒ",
            "ë²•ì •ê´€ë¦¬",
            "ê°ì‚¬ì˜ê²¬ ê±°ì ˆ",
            "ìë³¸ì ì‹",
            "ì–´ë‹ì‡¼í¬",
        ]

        non_financial_risk_keywords = [
            "íš¡ë ¹",
            "ë°°ì„",
            "ë¶„ì‹íšŒê³„",
            "ì†Œì†¡",
            "ê³µì •ìœ„ ì¡°ì‚¬",
            "ì••ìˆ˜ìˆ˜ìƒ‰",
            "ì˜ì—…ì •ì§€",
            "ì‚¬ì—… ì¤‘ë‹¨",
            "ê²½ì˜ì§„ ì‚¬í‡´",
            "êµ¬ì¡°ì¡°ì •",
        ]

        queries = [f"{base_query} ì‹ ìš©ìœ„í—˜", f"{base_query} ë¶€ë„ ê°€ëŠ¥ì„±"]

        for keyword in financial_risk_keywords:
            queries.append(f"{base_query} {keyword}")

        for keyword in non_financial_risk_keywords:
            queries.append(f"{base_query} {keyword}")

        return list(set(queries))

    def search_targeted_news(self, analysis_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ë¥¼ íƒ€ê²ŸíŒ… ê²€ìƒ‰
        Args:
            analysis_data: ì¬ë¬´ ë° ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼
        Returns:
            ê²€ìƒ‰ëœ ë‰´ìŠ¤ í•­ëª© ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        company_name = analysis_data.get("company_name", "ì‚¼ì„±ì „ì")

        print("ì¦ê°•ëœ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        search_keywords = self._generate_augmented_queries(company_name)
        print(f"  - ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(search_keywords)}ê°œ")

        search_keywords.append(f"{company_name} ë‰´ìŠ¤")

        all_news: List[Dict[str, Any]] = []

        for keyword in search_keywords:
            try:
                results = self.tavily_search.search(
                    query=keyword,
                    search_depth="advanced",
                    include_answer=False,
                    include_raw_content=False,
                    max_results=5,
                )

                if not results:
                    continue

                if isinstance(results, list):
                    for result in results:
                        news_item = self._process_search_result_clean(result, keyword)
                        if news_item:
                            all_news.append(news_item)
                else:
                    news_item = self._process_search_result_clean(results, keyword)
                    if news_item:
                        all_news.append(news_item)

            except Exception as e:
                print(f"   - ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {str(e)}")
                continue

        seen_urls = set()
        unique_news = []
        for news in all_news:
            url = news.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(news)

        return unique_news

    def _pre_filter_relevant_news(self, news_list: List[Dict[str, Any]], analysis_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        LLM ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§ìœ¼ë¡œ ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ë§Œ ì„ ë³„
        Args:
            news_list: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡
            analysis_data: ì´ìƒì¹˜ ë¶„ì„ ë°ì´í„°
        Returns:
            ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ ëª©ë¡ ë°˜í™˜
        """
        if not news_list:
            return []

        print("   - ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§ ì‹œì‘...")

        prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
                    You are a financial analyst's assistant. Your task is to quickly determine if a news article is potentially relevant for a company's credit risk assessment.
                    You will be given the company's name, a summary of detected financial and non-financial anomalies, and general credit risk definitions.
                    Based on this context, evaluate the provided news article. If the news article does not contain anomalies, but you determine it is related to the detected financial or non-financial anomalies, evaluate it as an anomaly.
                    Respond ONLY with a JSON object with two keys: "is_relevant" (boolean) and "reason" (a brief one-sentence explanation).
                    """,
                ),
                (
                    "human",
                    """
                    **Context for Assessment**
                    - Company Name: {company_name}
                    - Detected Financial Anomalies Summary: {financial_anomalies_summary}
                    - Detected Non-Financial Anomalies Summary: {non_financial_anomalies_summary}
                    - General Credit Risk Criteria: {credit_risk_criteria}

                    **News Article to Evaluate**
                    - Title: {news_title}
                    - Content: {news_content}

                    Is this news article potentially relevant for a credit risk assessment?
                    Provide your answer in JSON format.
                    """,
                ),
            ]
        )

        relevance_chain = prompt_template | self.llm | JsonOutputParser(pydantic_object=NewsRelevance)

        financial_anomalies = analysis_data.get("financial_anomalies", [])
        nfr_anomalies = analysis_data.get("non_financial_anomalies", [])

        financial_summary = (
            f"{len(financial_anomalies)} financial anomalies detected. "
            f"High severity indicators include: {', '.join([a['indicator'] for a in financial_anomalies if a.get('severity') == 'high'][:3])}"
        )
        nfr_summary = f"{len(nfr_anomalies)} non-financial anomalies detected."
        risk_criteria_summary = "Keywords include: revenue decline, debt increase, legal disputes, governance issues, market share loss."

        relevant_news: List[Dict[str, Any]] = []
        for news in news_list:
            try:
                result = relevance_chain.invoke(
                    {
                        "company_name": analysis_data.get("company_name"),
                        "financial_anomalies_summary": financial_summary,
                        "non_financial_anomalies_summary": nfr_summary,
                        "credit_risk_criteria": risk_criteria_summary,
                        "news_title": news.get("title"),
                        "news_content": news.get("content", "")[:800],
                    }
                )

                if isinstance(result, dict) and result.get("is_relevant"):
                    print(f"      - âœ… RELEVANT: {news.get('title')}")
                    relevant_news.append(news)
                else:
                    print(f"      - âŒ IRRELEVANT: {news.get('title')}")

            except Exception as e:
                print(f"      - âš ï¸ Pre-filtering error for news '{news.get('title')}': {e}")
                continue

        print(f"   - ì‚¬ì „ í•„í„°ë§ ì™„ë£Œ: {len(relevant_news)} / {len(news_list)} ë‰´ìŠ¤ ì„ ë³„")
        return relevant_news

    def _process_search_result_clean(self, result: Any, keyword: str) -> Dict[str, Any] | None:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€ ë‰´ìŠ¤ í•­ëª©ìœ¼ë¡œ ì •ë¦¬
        Args:
            result: Tavily ê²€ìƒ‰ ê²°ê³¼ ê°ì²´
            keyword: ê²€ìƒ‰ì— ì‚¬ìš©ëœ ì¿¼ë¦¬ ë¬¸ìì—´
        Returns:
            ì •ë¦¬ëœ ë‰´ìŠ¤ í•­ëª© ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        """
        try:
            if not isinstance(result, dict):
                return None

            title = self._clean_title(result.get("title", "ì œëª© ì—†ìŒ"))
            content = self._clean_content(result.get("content", result.get("snippet", "")))
            url = result.get("url", "")

            if len(content) < 30:
                return None

            return {
                "title": title,
                "url": url,
                "content": content,
                "published_date": result.get("published_date", result.get("date", "")),
                "search_keyword": keyword,
                "search_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        except Exception as e:
            print(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None

    def _clean_title(self, title: str) -> str:
        """ë‰´ìŠ¤ ì œëª© ì •ì œ"""

        if not title:
            return "ì œëª© ì—†ìŒ"

        import re

        title = re.sub(r"<[^>]+>", "", title)
        title = title.replace("\n", " ").replace("\t", " ")
        title = re.sub(r"\s+", " ", title).strip()

        if len(title) > 100:
            title = title[:100] + "..."

        return title

    def _clean_content(self, content: str) -> str:
        """ë‰´ìŠ¤ ë³¸ë¬¸ ì •ì œ"""

        if not content:
            return "ë‚´ìš© ì—†ìŒ"

        import re

        content = re.sub(r"<[^>]+>", "", content)
        content = re.sub(r"javascript:[^\"']*", "", content)
        content = re.sub(r"https?://[^\s]+", "", content)

        unwanted_patterns = [
            r"ì´ë¯¸ì§€.*?jpg|png|gif",
            r"ê²€ìƒ‰.*?í•˜ê¸°",
            r"í˜ì´ì§€.*?ë§í¬",
            r"Copyright.*?\d{4}",
            r"ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸.*?\d",
        ]
        for pattern in unwanted_patterns:
            content = re.sub(pattern, "", content, flags=re.IGNORECASE)

        content = content.replace("\n", " ").replace("\t", " ")
        content = re.sub(r"\s+", " ", content).strip()

        if len(content) > 1000:
            content = content[:1000] + "..."

        return content

    def create_enhanced_risk_assessment_prompt(self) -> ChatPromptTemplate:
        """
        ì´ìƒì¹˜ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì‹ ìš©ìœ„í—˜ íŒë‹¨ í”„ë¡¬í”„íŠ¸ ìƒì„±
        Returns:
            ìœ„í—˜ í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë°˜í™˜
        """
        system_message = """
                        ë‹¹ì‹ ì€ ì‹ ìš©ìœ„í—˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì—…ì˜ ì¬ë¬´/ë¹„ì¬ë¬´ ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ê³ ë ¤í•˜ì—¬ ë‰´ìŠ¤ê°€ ì‹ ìš©ìœ„í—˜ ì§•í›„ì¸ì§€ íŒë‹¨í•˜ê³ , êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

                        **ì¤‘ìš”í•œ íŒë‹¨ ì›ì¹™**:
                        1. **ì„ ë³„ëœ ë‰´ìŠ¤ ë¶„ì„**: ì…ë ¥ëœ ë‰´ìŠ¤ëŠ” ì´ë¯¸ ì´ìƒ ì§•í›„ ë° ì‹ ìš©ìœ„í—˜ ê¸°ì¤€ì— ë”°ë¼ 1ì°¨ ì„ ë³„ëœ ê²ƒì…ë‹ˆë‹¤. ì´ ë‰´ìŠ¤ë“¤ì´ ì™œ, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜ ìœ„í—˜í•œì§€ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
                        2. **ëª…í™•í•œ ê·¼ê±° ê¸°ë°˜ íŒë‹¨**: ê° ë‰´ìŠ¤ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì„¤ëª… ê°€ëŠ¥í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
                        3. **ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„± ì¤‘ì‹œ**: íƒì§€ëœ ì¬ë¬´/ë¹„ì¬ë¬´ ì´ìƒì¹˜ì™€ ë‰´ìŠ¤ì˜ ì—°ê´€ì„±ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
                        4. **ë³µí•©ì  ìœ„í—˜ í‰ê°€**: ë‰´ìŠ¤ ë‹¨ë…ìœ¼ë¡œëŠ” ì •ìƒì´ì§€ë§Œ ì´ìƒì¹˜ì™€ í•¨ê»˜ ë³´ë©´ ìœ„í—˜í•œ ê²½ìš°ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
                        5. **ì •ëŸ‰ì  ìœ„í—˜ë„ í‰ê°€**: ìœ„í—˜ë„ë¥¼ 1-10ì ìœ¼ë¡œ ìˆ˜ì¹˜í™”í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”.

                        **ë³µí•© ìœ„í—˜ íŒë‹¨ ì˜ˆì‹œ**:
                        - ë‰´ìŠ¤: "ê¸°ì—…ì´ ìƒˆë¡œìš´ ì‚¬ì—…ì„ ì‹œì‘"
                        - ì¬ë¬´ ì´ìƒì¹˜: "ë¶€ì±„ë¹„ìœ¨ì´ ë†’ìŒ"
                        - íŒë‹¨: ë¶€ì±„ë¹„ìœ¨ì´ ë†’ì€ ìƒí™©ì—ì„œ ì‹ ê·œ ì‚¬ì—… íˆ¬ìëŠ” ì¬ë¬´ ë¶€ë‹´ ê°€ì¤‘ìœ¼ë¡œ ìœ„í—˜ (ìœ„í—˜ë„ 7ì )

                        **íƒì§€ëœ ì´ìƒì¹˜**:
                        ì¬ë¬´ ì´ìƒì¹˜: {financial_anomalies_summary}
                        ë¹„ì¬ë¬´ ì´ìƒì¹˜: {non_financial_anomalies_summary}
                        í˜„ì¬ ì‹ ìš©ë“±ê¸‰: {current_credit_grade}

                        **ì‹ ìš©ìœ„í—˜ ì§•í›„ ê¸°ì¤€**:
                        {credit_risk_criteria}

                        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
                        {{
                            "analysis_date": {analysis_date},
                            "company_name": "ê¸°ì—…ëª…",
                            "total_risk_news": ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
                            "high_risk_count": ê³ ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
                            "medium_risk_count": ì¤‘ìœ„í—˜ë‰´ìŠ¤ìˆ˜,
                            "key_risk_summary": "ì£¼ìš” ìœ„í—˜ ìš”ì•½ (2-3ë¬¸ì¥)",
                            "risk_news_items": [
                                {{
                                    "title": "ë‰´ìŠ¤ ì œëª©",
                                    "content_summary": "ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½ (1-2ë¬¸ì¥)",
                                    "url": "ë‰´ìŠ¤ URL",
                                    "risk_category": "ì¬ë¬´ìœ„í—˜/ë¹„ì¬ë¬´ìœ„í—˜/ë³µí•©ìœ„í—˜",
                                    "risk_level": ìœ„í—˜ë„(1-10),
                                    "evidence_basis": "ìœ„í—˜ íŒë‹¨ ê·¼ê±° (êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ)",
                                    "anomaly_connection": "íƒì§€ëœ ì´ìƒì¹˜ì™€ì˜ ì—°ê´€ì„±",
                                    "published_date": "ë°œí–‰ì¼"
                                }}
                            ],
                            "credit_assessment": {{
                                "current_grade": "í˜„ì¬ ì¶”ì • ì‹ ìš©ë“±ê¸‰",
                                "predicted_grade": "ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰",
                                "change_probability": ë³€ê²½í™•ë¥ (0-1),
                                "change_direction": "ìƒìŠ¹/í•˜ë½/ìœ ì§€",
                                "reasoning": "ë“±ê¸‰ í‰ê°€ ê·¼ê±°"
                            }}
                        }}

                        **ìœ„í—˜ë„ í‰ê°€ ê¸°ì¤€**:
                        - 9-10ì : ì‹¬ê°í•œ ìœ„í—˜ (ì‹ ìš©ë“±ê¸‰ ì¦‰ì‹œ í•˜ë½ ìš”ì¸)
                        - 7-8ì : ë†’ì€ ìœ„í—˜ (ë‹¨ê¸°ë‚´ ì‹ ìš©ë“±ê¸‰ì— ë¶€ì •ì  ì˜í–¥)
                        - 5-6ì : ì¤‘ê°„ ìœ„í—˜ (ì§€ì† ëª¨ë‹ˆí„°ë§ í•„ìš”)
                        - 3-4ì : ë‚®ì€ ìœ„í—˜ (ì ì¬ì  ìš°ë ¤ì‚¬í•­)
                        - 1-2ì : ë¯¸ë¯¸í•œ ìœ„í—˜ (ì¼ë°˜ì  ë¹„ì¦ˆë‹ˆìŠ¤ í™œë™)

                        **ì¤‘ìš”**: ìœ„í—˜ë„ 5ì  ì´ìƒì¸ ë‰´ìŠ¤ë§Œ risk_news_itemsì— í¬í•¨í•˜ì„¸ìš”. ìœ„í—˜ ë‰´ìŠ¤ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° risk_news_itemsë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
                        """

        human_message = """
                        ## íƒì§€ëœ ì´ìƒì¹˜ ì •ë³´

                        ### ì¬ë¬´ ì´ìƒì¹˜
                        {financial_anomalies_detail}

                        ### ë¹„ì¬ë¬´ ì´ìƒì¹˜  
                        {non_financial_anomalies_detail}

                        ### ê¸°ì—… ì¬ë¬´ íŠ¹ì„±
                        {financial_characteristics}

                        ## ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤ (1ì°¨ ì„ ë³„ë¨)
                        {news_data}

                        ìœ„ ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ìš©ìœ„í—˜ ì§•í›„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  êµ¬ì¡°í™”ëœ JSON ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
                        """

        return ChatPromptTemplate.from_messages([("system", system_message), ("human", human_message)])

    def assess_credit_risk_with_anomalies(self, analysis_data: Dict[str, Any], news_list: List[Dict[str, Any]]) -> DailyRiskSummary:
        """
        ì´ìƒì¹˜ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ ì‹ ìš©ìœ„í—˜ ì§•í›„ í‰ê°€ ìˆ˜í–‰
        Args:
            analysis_data: ì¬ë¬´ ë° ë¹„ì¬ë¬´ ë¶„ì„ ê²°ê³¼
            news_list: ì‚¬ì „ í•„í„°ë§ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        Returns:
            ì¼ì¼ ìœ„í—˜ ìš”ì•½ ê²°ê³¼ ë°˜í™˜
        """
        if not news_list:
            return DailyRiskSummary(
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                company_name=analysis_data.get("company_name", "Unknown"),
                total_risk_news=0,
                high_risk_count=0,
                medium_risk_count=0,
                key_risk_summary="ì˜¤ëŠ˜ ê´€ë ¨ëœ ìœ„í—˜ ë‰´ìŠ¤ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ì‚¬ì „ í•„í„°ë§ ê²°ê³¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ)",
                risk_news_items=[],
                credit_assessment=CreditAssessment(
                    current_grade=analysis_data.get("current_credit_grade", "A"),
                    predicted_grade=analysis_data.get("current_credit_grade", "A"),
                    change_probability=0.0,
                    change_direction="ìœ ì§€",
                    reasoning="ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ ë¶€ì¬ë¡œ ë“±ê¸‰ ìœ ì§€ ì˜ˆìƒ",
                ),
            )

        try:
            prompt = self.create_enhanced_risk_assessment_prompt()
            json_parser = JsonOutputParser(pydantic_object=DailyRiskSummary)
            chain = prompt | self.llm | json_parser

            news_text = "\n\n".join(
                [
                    (
                        f"[ë‰´ìŠ¤ {i + 1}]\n"
                        f"ì œëª©: {news.get('title', '')}\n"
                        f"ë‚´ìš©: {news.get('content', '')[:500]}...\n"
                        f"URL: {news.get('url', 'URL ì—†ìŒ')}\n"
                        f"ë°œí–‰ì¼: {news.get('published_date', '')}"
                    )
                    for i, news in enumerate(news_list[:8])
                ]
            )

            financial_detail = "\n".join(
                [
                    f"- {anomaly.get('indicator', '')}: {anomaly.get('description', '')} (ì‹¬ê°ë„: {anomaly.get('severity', '')})"
                    for anomaly in analysis_data.get("financial_anomalies", [])
                ]
            ) or "íƒì§€ëœ ì¬ë¬´ ì´ìƒì¹˜ ì—†ìŒ"

            nfr_detail = "\n".join(
                [
                    f"- {anomaly.get('indicator', '')} ({anomaly.get('pillar', '')}): {anomaly.get('description', '')} (ì‹¬ê°ë„: {anomaly.get('severity', '')})"
                    for anomaly in analysis_data.get("non_financial_anomalies", [])[:10]
                ]
            ) or "íƒì§€ëœ ë¹„ì¬ë¬´ ì´ìƒì¹˜ ì—†ìŒ"

            financial_summary = f"{len(analysis_data.get('financial_anomalies', []))}ê°œ ì¬ë¬´ ì´ìƒì¹˜"
            nfr_summary = f"{len(analysis_data.get('non_financial_anomalies', []))}ê°œ ë¹„ì¬ë¬´ ì´ìƒì¹˜"

            result = chain.invoke(
                {
                    "analysis_date": datetime.now().strftime("%Y-%m-%d"),
                    "financial_anomalies_summary": financial_summary,
                    "non_financial_anomalies_summary": nfr_summary,
                    "current_credit_grade": analysis_data.get("current_credit_grade", "A"),
                    "credit_risk_criteria": json.dumps(self.credit_risk_criteria, ensure_ascii=False, indent=1),
                    "financial_anomalies_detail": financial_detail,
                    "non_financial_anomalies_detail": nfr_detail,
                    "financial_characteristics": json.dumps(analysis_data.get("financial_characteristics", {}), ensure_ascii=False),
                    "news_data": news_text,
                }
            )

            if isinstance(result, dict):
                return DailyRiskSummary(**result)

            return result

        except Exception as e:
            print(f"âš ï¸ LLM API ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return DailyRiskSummary(
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                company_name=analysis_data.get("company_name", "Unknown"),
                total_risk_news=0,
                high_risk_count=0,
                medium_risk_count=0,
                key_risk_summary=f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                risk_news_items=[],
                credit_assessment=CreditAssessment(
                    current_grade=analysis_data.get("current_credit_grade", "A"),
                    predicted_grade=analysis_data.get("current_credit_grade", "A"),
                    change_probability=0.0,
                    change_direction="ìœ ì§€",
                    reasoning="ë¶„ì„ ì˜¤ë¥˜ë¡œ ë“±ê¸‰ ë³€ê²½ ì˜ˆì¸¡ ë¶ˆê°€",
                ),
            )

    def generate_daily_summary_report(self, risk_summary: DailyRiskSummary) -> str:
        """
        ì¼ì¼ ì‹ ìš©ìœ„í—˜ ì•Œë¦¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        Args:
            risk_summary: ì¼ì¼ ìœ„í—˜ ë¶„ì„ ê²°ê³¼
        Returns:
            ì•Œë¦¼ í˜•ì‹ ìš”ì•½ ë¦¬í¬íŠ¸ ë¬¸ìì—´ ë°˜í™˜
        """
        report = (
            f"ğŸš¨ {risk_summary.company_name} ì‹ ìš©ìœ„í—˜ ì•Œë¦¼ ìš”ì•½\n\n"
            f"    ğŸ“… ë¶„ì„ì¼ì: {risk_summary.analysis_date}\n"
            f"    ğŸ“Š ìœ„í—˜ ë‰´ìŠ¤: {risk_summary.total_risk_news}ê°œ (ê³ ìœ„í—˜ {risk_summary.high_risk_count}ê°œ, ì¤‘ìœ„í—˜ {risk_summary.medium_risk_count}ê°œ)\n"
            f"    âš–ï¸ ì‹ ìš©ë“±ê¸‰: {risk_summary.credit_assessment.current_grade} â†’ {risk_summary.credit_assessment.predicted_grade} ({risk_summary.credit_assessment.change_probability:.1%})\n\n"
        )

        if risk_summary.risk_news_items:
            sorted_news = sorted(risk_summary.risk_news_items, key=lambda x: x.risk_level, reverse=True)

            for news in sorted_news:
                if news.risk_level >= 8:
                    risk_emoji = "ğŸ”¥"
                    alert_level = "ê¸´ê¸‰"
                elif news.risk_level >= 6:
                    risk_emoji = "âš ï¸"
                    alert_level = "ì£¼ì˜"
                else:
                    risk_emoji = "â„¹ï¸"
                    alert_level = "ëª¨ë‹ˆí„°ë§"

                report += (
                    "\n"
                    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                    f"    {risk_emoji} {news.risk_category} ë¦¬ìŠ¤í¬ {alert_level} ê²½ê³ \n"
                    f'    [{news.published_date}] "{news.title}"\n\n'
                )

                if news.anomaly_connection and news.anomaly_connection.strip() != "í•´ë‹¹ì—†ìŒ":
                    report += (
                        "ğŸ” ì´ìƒì§•í›„ íƒì§€ ê·¼ê±°:\n"
                        f"    {news.anomaly_connection}\n\n"
                        "    ğŸ“ˆ ë‰´ìŠ¤ ë¶„ì„:\n"
                        f"    {news.content_summary}\n\n"
                        "    ğŸ¤– AI ìœ„í—˜ë„ í‰ê°€:\n"
                        f"    {news.evidence_basis}\n"
                        f"    (ìœ„í—˜ë„: {news.risk_level}/10)\n\n"
                    )
                else:
                    report += (
                        "    ğŸ“ˆ ìƒí™© ë¶„ì„:\n"
                        f"    {news.content_summary}\n\n"
                        "    ğŸ¤– AI ìœ„í—˜ë„ í‰ê°€:\n"
                        f"    {news.evidence_basis}\n"
                        f"    (ìœ„í—˜ë„: {news.risk_level}/10)\n\n"
                    )
        else:
            report += (
                "\n"
                "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "    âœ… ì˜¤ëŠ˜ íƒì§€ëœ ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤\n\n"
            )

        report += (
            "\n"
            "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            "    ğŸ“‹ ì¢…í•© ìœ„í—˜ë„ í‰ê°€\n\n"
            f"    {risk_summary.key_risk_summary}\n\n"
            "    ğŸ’¡ ì‹ ìš©ë“±ê¸‰ ë³€ë™ ì „ë§:\n"
            f"    í˜„ì¬ {risk_summary.credit_assessment.current_grade}ë“±ê¸‰ì—ì„œ {risk_summary.credit_assessment.predicted_grade}ë“±ê¸‰ìœ¼ë¡œ "
            f"{risk_summary.credit_assessment.change_direction}í•  í™•ë¥ ì´ {risk_summary.credit_assessment.change_probability:.1%}ì…ë‹ˆë‹¤\n\n"
            f"    ê·¼ê±°: {risk_summary.credit_assessment.reasoning}\n\n"
            "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
            f"    â° ì•Œë¦¼ ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"    ğŸ”„ ë‹¤ìŒ ëª¨ë‹ˆí„°ë§: {(datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')}\n"
        )

        return report

    def analyze_credit_risk_with_results(self, result_dir: str) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ìš©ìœ„í—˜ ì§•í›„ ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        Args:
            result_dir: ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        """
        print("=== ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘ ===")

        print("1. ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        analysis_data = self.load_analysis_results(result_dir)

        company_name = analysis_data.get("company_name", "Unknown")
        financial_anomalies_count = len(analysis_data.get("financial_anomalies", []))
        nfr_anomalies_count = len(analysis_data.get("non_financial_anomalies", []))

        print(f"   - ê¸°ì—…ëª…: {company_name}")
        print(f"   - ì¬ë¬´ ì´ìƒì¹˜: {financial_anomalies_count}ê°œ")
        print(f"   - ë¹„ì¬ë¬´ ë¶„ì„ í•­ëª©: {nfr_anomalies_count}ê°œ")

        print("\n2. ì´ìƒì¹˜ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        targeted_news = self.search_targeted_news(analysis_data)
        print(f"   - ê²€ìƒ‰ëœ ì´ ë‰´ìŠ¤ ìˆ˜: {len(targeted_news)}ê°œ")

        print("\n2.5. ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤ ì‚¬ì „ í•„í„°ë§ ì¤‘...")
        relevant_news = self._pre_filter_relevant_news(targeted_news, analysis_data)

        print("\n3. ì‹ ìš©ìœ„í—˜ ì§•í›„ ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        risk_summary = self.assess_credit_risk_with_anomalies(analysis_data, relevant_news)

        print("\n4. ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        daily_report = self.generate_daily_summary_report(risk_summary)

        analysis_result = {
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "company": company_name,
            "total_news_count": len(targeted_news),
            "relevant_news_count": len(relevant_news),
            "financial_anomalies_count": financial_anomalies_count,
            "non_financial_anomalies_count": nfr_anomalies_count,
            "company_analysis_summary": self._create_analysis_summary(analysis_data),
            "relevant_news": relevant_news,
            "daily_risk_summary": risk_summary.dict(),
            "daily_summary_report": daily_report,
            "analysis_basis": "detected_anomalies_and_pre_filtered_news",
        }

        try:
            json_output_file = os.path.join(result_dir, "daily_news_risk_analysis.json")
            with open(json_output_file, "w", encoding="utf-8") as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… JSON ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_output_file}")

            report_output_file = os.path.join(result_dir, "daily_risk_summary.md")
            with open(report_output_file, "w", encoding="utf-8") as f:
                f.write(daily_report)
            print(f"âœ… ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_output_file}")

        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

        print("\n=== ë¶„ì„ ì™„ë£Œ ===")
        return analysis_result

    def _create_analysis_summary(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ìƒì¹˜ ê¸°ë°˜ ìœ„í—˜ ìš”ì•½ ì •ë³´ ìƒì„±"""

        financial_anomalies = analysis_data.get("financial_anomalies", [])
        nfr_anomalies = analysis_data.get("non_financial_anomalies", [])

        financial_high_risk = sum(1 for a in financial_anomalies if a.get("severity") in ["high", "critical"])
        financial_medium_risk = sum(1 for a in financial_anomalies if a.get("severity") == "medium")

        nfr_high_risk = sum(1 for a in nfr_anomalies if a.get("severity") in ["high"])
        nfr_medium_risk = sum(1 for a in nfr_anomalies if a.get("severity") == "medium")

        total_high_risk = financial_high_risk + nfr_high_risk
        total_anomalies = len(financial_anomalies) + len(nfr_anomalies)

        if total_high_risk >= 3 or total_anomalies >= 10:
            overall_risk = "ë†’ìŒ"
        elif total_high_risk >= 1 or total_anomalies >= 6:
            overall_risk = "ì£¼ì˜"
        elif total_anomalies >= 3:
            overall_risk = "ë³´í†µ"
        else:
            overall_risk = "ë‚®ìŒ"

        concerns = []
        for anomaly in financial_anomalies:
            if anomaly.get("severity") in ["high", "medium", "critical"]:
                concerns.append(f"ì¬ë¬´: {anomaly.get('description', '')}")

        for anomaly in nfr_anomalies:
            if anomaly.get("severity") in ["high", "medium"]:
                concerns.append(f"ë¹„ì¬ë¬´({anomaly.get('pillar', '')}): {anomaly.get('description', '')}")

        return {
            "overall_risk_level": overall_risk,
            "total_anomalies": total_anomalies,
            "high_risk_anomalies": total_high_risk,
            "financial_risk_count": len(financial_anomalies),
            "non_financial_risk_count": len(nfr_anomalies),
            "financial_characteristics": analysis_data.get("financial_characteristics", {}),
            "key_concerns": concerns[:5],
        }


def main() -> None:
    """ì´ìƒì¹˜ ê¸°ë°˜ ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ ë¶„ì„ ì‹¤í–‰"""

    try:
        from dotenv import load_dotenv

        load_dotenv()

        if not os.getenv("OPENAI_API_KEY"):
            print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        if not os.getenv("TAVILY_API_KEY"):
            print("âš ï¸ TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        print("=== ì‹ ìš©ìœ„í—˜ ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ===")
        analyzer = CreditRiskNewsAnalyzer(max_search_results=5)

        test_result_dir = "analysis_results/ì‚¼ì„±ì „ì"
        if not os.path.exists(test_result_dir):
            os.makedirs(test_result_dir)
            print(f"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±: {test_result_dir}")

        print("\n=== ì´ìƒì¹˜ ê¸°ë°˜ ì‹ ìš©ìœ„í—˜ ì§•í›„ ë¶„ì„ ì‹¤í–‰ ===")
        analysis_result = analyzer.analyze_credit_risk_with_results(test_result_dir)

        print("\n" + "=" * 60)
        print("ğŸ“° ì‹ ìš©ìœ„í—˜ ì§•í›„ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ")
        print("=" * 60)

        risk_summary = analysis_result.get("daily_risk_summary", {})

        print("ğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"- ë¶„ì„ ëŒ€ìƒ: {analysis_result['company']}")
        print(f"- ì´ ê²€ìƒ‰ëœ ë‰´ìŠ¤: {analysis_result['total_news_count']}ê°œ")
        print(f"- ì‹ ìš©ìœ„í—˜ ê´€ë ¨ ë‰´ìŠ¤: {analysis_result['relevant_news_count']}ê°œ (ì„ ë³„ë¨)")
        print(f"- ìµœì¢… ë¶„ì„ëœ ìœ„í—˜ ë‰´ìŠ¤: {risk_summary.get('total_risk_news', 0)}ê°œ")
        print(f"  - ê³ ìœ„í—˜: {risk_summary.get('high_risk_count', 0)}ê°œ, ì¤‘ìœ„í—˜: {risk_summary.get('medium_risk_count', 0)}ê°œ")

        credit_assessment = risk_summary.get("credit_assessment", {})
        print("\nğŸ“ˆ ì‹ ìš©ë“±ê¸‰ í‰ê°€:")
        print(f"- í˜„ì¬ ì‹ ìš©ë“±ê¸‰: {credit_assessment.get('current_grade', 'Unknown')}")
        print(f"- ì˜ˆìƒ ì‹ ìš©ë“±ê¸‰: {credit_assessment.get('predicted_grade', 'Unknown')}")
        print(f"- ë³€ê²½ í™•ë¥ : {credit_assessment.get('change_probability', 0):.1%}")
        print(f"- ë³€ê²½ ë°©í–¥: {credit_assessment.get('change_direction', 'Unknown')}")

        print("\nğŸ“„ ìƒì„±ëœ íŒŒì¼:")
        print(f"- {os.path.join(test_result_dir, 'daily_news_risk_analysis.json')}")
        print(f"- {os.path.join(test_result_dir, 'daily_risk_summary.md')}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
