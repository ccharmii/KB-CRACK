# explainer.py
# -*- coding: utf-8 -*-
import os, json, time, re
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

from .indexer import build_or_load_faiss, get_retriever
from .config import CHAT_MODEL

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ====== ì„¤ì • ======
SNIPPET_MAX    = 420     # ìŠ¤ë‹ˆí« ìµœëŒ€ ê¸¸ì´
TOP_K_DEFAULT  = 8       # ì§€í‘œë³„ ë¦¬íŠ¸ë¦¬ë¸Œ ë¬¸ë§¥ ìˆ˜
# ==================

# ---------------- ê³µìš© ìœ í‹¸/ë¡œê¹… ----------------
def _log(msg, verbose=True):
    if not verbose: return
    ts = time.strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)

def _ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True)

def _save_json(path, data, verbose=True):
    _ensure_dir(Path(path).parent)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    # _log(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {os.path.abspath(path)}", verbose)
    return path

def _corp_root(script_dir, corp_code):
    return os.path.join(".\data", corp_code)

def _db_path(corp_root):
    return os.path.join(corp_root, "nfr.db")

# ---------------- DBì—ì„œ ìµœì‹  ë¶„ê¸° ----------------
def _connect(db_path, verbose=True):
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"DBê°€ ì—†ìŠµë‹ˆë‹¤: {db_path}")
    import sqlite3
    _log(f"ğŸ—„ï¸  DB ì—°ê²°: {db_path}", verbose)
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn

def _latest_quarter(conn, corp_code, verbose=True):
    row = conn.execute(
        "SELECT quarter FROM chunks WHERE corp_code=? AND quarter IS NOT NULL AND quarter<>'' "
        "ORDER BY quarter DESC LIMIT 1",
        (corp_code,)
    ).fetchone()
    q = row["quarter"] if row else None
    _log(f"ğŸ“… ìµœì‹  ë¶„ê¸° ê°ì§€: {q or 'ì—†ìŒ'}", verbose)
    return q

# ---------------- ì§€í‘œ â†’ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ----------------
# (ì •ê·œì‹ìœ¼ë¡œ ê´‘ë²”ìœ„ ë§¤í•‘: ì–´ë–¤ ì§€í‘œëª…ì´ ì™€ë„ ê°€ì¥ ê°€ê¹Œìš´ ë²„í‚·ìœ¼ë¡œ ë¶„ë¥˜)
_BUCKET_RULES = [
    ("profit",  r"(ìˆœì´ìµë¥ |ì´í¬ê´„ì´ìµë¥ |ë§¤ì¶œì´ì´ìµë¥ |ë§¤ì¶œì›ê°€ìœ¨|íŒê´€ë¹„ìœ¨|ì˜ì—…ìˆ˜ìµê²½ë¹„ìœ¨|ì„¸ì „ê³„ì†ì‚¬ì—…ì´ìµë¥ |ROE|ìë³¸ê¸ˆì˜ì—…ì´ìµë¥ |ìê¸°ìë³¸ì˜ì—…ì´ìµë¥ |ì´ìì‚°ì˜ì—…ì´ìµë¥ )"),
    ("growth",  r"(ì¦ê°€ìœ¨|YoY|yoy|ì „ë…„ë™ê¸°|ì „ë…„ ëŒ€ë¹„|ì„±ì¥ë¥ )"),
    ("leverage",r"(ë¶€ì±„ë¹„ìœ¨|ìê¸°ìë³¸ë¹„ìœ¨|ì¬ë¬´ë ˆë²„ë¦¬ì§€|ê¸ˆìœµë¹„ìš©ë¶€ë‹´ë¥ |ìœ ë™ë¹„ìœ¨|ìœ ë™ë¶€ì±„ë¹„ìœ¨|ë¹„ìœ ë™ë¶€ì±„ë¹„ìœ¨|ë¹„ìœ ë™ë¹„ìœ¨|ë¹„ìœ ë™ì í•©ë¥ |ìœ ë™ì„±)"),
    ("mix",     r"(ìì‚°êµ¬ì„±ë¹„ìœ¨|ë¹„ìœ ë™ìì‚°êµ¬ì„±ë¹„ìœ¨|ìœ í˜•ìì‚°êµ¬ì„±ë¹„ìœ¨|ìœ ë™ìì‚°êµ¬ì„±ë¹„ìœ¨|ì¬ê³ ìì‚°êµ¬ì„±ë¹„ìœ¨|ìœ ë™ìì‚°/ë¹„ìœ ë™ìì‚°ë¹„ìœ¨|ì¬ê³ ìì‚°/ìœ ë™ìì‚°ë¹„ìœ¨)"),
    ("assetchg",r"(ì´ìì‚°ì¦ê°€ìœ¨|ë¹„ìœ ë™ìì‚°ì¦ê°€ìœ¨|ìœ í˜•ìì‚°ì¦ê°€ìœ¨|ë¶€ì±„ì´ê³„ì¦ê°€ìœ¨|ìê¸°ìë³¸ì¦ê°€ìœ¨|ìœ ë™ìì‚°ì¦ê°€ìœ¨|ì¬ê³ ìì‚°ì¦ê°€ìœ¨|ìœ ë™ë¶€ì±„ì¦ê°€ìœ¨|ë¹„ìœ ë™ë¶€ì±„ì¦ê°€ìœ¨)"),
    ("eff",     r"(íšŒì „ìœ¨|ì¬ê³ ìì‚°íšŒì „ìœ¨|ì´ìì‚°íšŒì „ìœ¨|ë¹„ìœ ë™ìì‚°íšŒì „ìœ¨|ìœ í˜•ìì‚°íšŒì „ìœ¨|íƒ€ì¸ìë³¸íšŒì „ìœ¨|ìê¸°ìë³¸íšŒì „ìœ¨|ìë³¸ê¸ˆíšŒì „ìœ¨|ì´ìë³¸íšŒì „ìœ¨|ë§¤ì¶œì›ê°€/ì¬ê³ ìì‚°)"),
    ("valuation",r"(PER|PBR|EPS|ë°°ë‹¹ì„±í–¥|market_cap|ì‹œê°€ì´ì•¡|close_price|ì£¼ê°€|ì‹œì´)"),
]

_EXTRAS_BY_BUCKET = {
    "profit":   ["ë§ˆì§„", "ë‹¨ê°€", "íŒê°€", "ì›ê°€", "ë¹„ìš©", "ê°ê°€ìƒê°", "ì¶©ë‹¹ê¸ˆ", "ì†ìƒì°¨ì†", "íŒë§¤ë¯¹ìŠ¤", "ê°€ë™ë¥ ", "ìˆ˜ìœ¨", "í™˜ìœ¨", "ì¼íšŒì„±", "ë¦¬ì½œ", "ì†Œì†¡"],
    "growth":   ["ìˆ˜ìš”", "ì¶œí•˜", "ìˆ˜ì£¼", "ê°€ë™ë¥ ", "ì¦ì„¤", "ìƒì‚°ëŠ¥ë ¥", "ì‹ ê·œê³ ê°", "ì‹ ì œí’ˆ", "ê°€ê²©ì „ê°€", "í™˜ìœ¨", "ì±„ë„ì¬ê³ ", "ì‹œì¥ì ìœ ìœ¨"],
    "leverage": ["ì°¨ì…", "ì´ìë¹„ìš©", "íšŒì‚¬ì±„", "ë§Œê¸°", "í˜„ê¸ˆíë¦„", "ìš´ì „ìë³¸", "ì°¨í™˜", "ë‹´ë³´", "ìœ ë™ì„±", "ê¸ˆë¦¬", "ë“±ê¸‰", "ì½”ë²„ë„ŒíŠ¸"],
    "mix":      ["ìì‚° êµ¬ì„±", "CAPEX", "ì„¤ë¹„íˆ¬ì", "ì·¨ë“", "ë§¤ê°", "ì¬ê³ ", "í‰ê°€ì†ìµ", "ê°ê°€ìƒê°", "ìœ í˜•ìì‚°", "ë¬´í˜•ìì‚°"],
    "assetchg": ["ì¦ê°€ ì‚¬ìœ ", "ê°ì†Œ ì‚¬ìœ ", "ì·¨ë“", "ë§¤ê°", "ìœ ìƒì¦ì", "ë°°ë‹¹", "í™˜ìœ¨ ì˜í–¥", "í‰ê°€ì†ìµ", "ì†ìƒ", "ì¶©ë‹¹ê¸ˆ"],
    "eff":      ["ì¬ê³ íšŒì „", "ë§¤ì¶œì±„ê¶Œ", "ë§¤ì…ì±„ë¬´", "ìš´ì „ìë³¸", "ì¬ê³ ì¼ìˆ˜", "í˜„ê¸ˆíšŒì „", "ë¦¬ë“œíƒ€ì„", "ìˆ˜ìœ¨", "ê°€ë™ë¥ "],
    "valuation":["ì£¼ê°€", "ì‹œê°€ì´ì•¡", "íˆ¬ìì‹¬ë¦¬", "ë©€í‹°í”Œ", "ê°€ì´ë˜ìŠ¤", "ì‹¤ì ì „ë§", "ê·œì œ", "ì†Œì†¡", "ë¦¬ì½œ", "ë‰´ìŠ¤", "ìˆ˜ê¸‰"],
}

# ê°œë³„ ì§€í‘œì˜ ì˜ë¬¸/ë™ì˜ì–´ ë³´ê°•(ìˆìœ¼ë©´ ì¶”ê°€ë¡œ ë¶™ì„)
_SYNONYM_BY_METRIC = {
    "ROE": ["Return on Equity", "ìê¸°ìë³¸ì´ìµë¥ ", "ROE"],
    "PER": ["PER", "Price Earnings Ratio", "ë©€í‹°í”Œ"],
    "PBR": ["PBR", "Price to Book"],
    "EPS": ["EPS", "Earnings per Share"],
    "YoY": ["YoY", "Year over Year", "ì „ë…„ë™ê¸° ëŒ€ë¹„", "ì „ë…„ ëŒ€ë¹„"],
}

_GENERIC_EXTRAS = ["ì›ì¸", "ìš”ì¸", "ì‚¬ìœ ", "ë°°ê²½", "ë³€ë™", "ì¦ê°€", "ê°ì†Œ", "ê°€ê²©", "ì¬ê³ ", "í™˜ìœ¨", "ìˆ˜ìš”", "ê³µê¸‰", "ê²½ìŸ", "ì •ì±…", "ê·œì œ"]

def _bucketize_metric(metric_name):
    m = metric_name or ""
    for bucket, pat in _BUCKET_RULES:
        if re.search(pat, m, flags=re.I):
            return bucket
    # ë””í´íŠ¸: ë‚´ìš©ì— ë”°ë¼ ì ë‹¹íˆ í¬ê´„ ë²„í‚·
    if re.search(r"(PER|PBR|EPS|ì‹œê°€ì´ì•¡|market_cap|ì£¼ê°€|close_price)", m, flags=re.I): return "valuation"
    if "íšŒì „" in m: return "eff"
    if "ì¦ê°€ìœ¨" in m or "YoY" in m: return "growth"
    return "profit"

def _direction_terms(anomaly_text):
    t = anomaly_text or ""
    terms = []
    if any(k in t for k in ["ìƒíšŒ", "ì¦ê°€", "í™•ëŒ€", "ê°œì„ ", "í˜¸ì „"]): terms += ["ì¦ê°€", "í™•ëŒ€", "ê°œì„ "]
    if any(k in t for k in ["í•˜íšŒ", "ê°ì†Œ", "ì¶•ì†Œ", "ì•…í™”", "ë‘”í™”"]): terms += ["ê°ì†Œ", "ì¶•ì†Œ", "ì•…í™”", "ë‘”í™”"]
    return list(dict.fromkeys(terms))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€

def _metric_synonyms(metric_name):
    n = metric_name or ""
    out = []
    if re.search(r"ROE", n, re.I): out += _SYNONYM_BY_METRIC["ROE"]
    if re.search(r"PER", n, re.I): out += _SYNONYM_BY_METRIC["PER"]
    if re.search(r"PBR", n, re.I): out += _SYNONYM_BY_METRIC["PBR"]
    if re.search(r"EPS", n, re.I): out += _SYNONYM_BY_METRIC["EPS"]
    if re.search(r"(YoY|ì „ë…„|ì¦ê°€ìœ¨)", n, re.I): out += _SYNONYM_BY_METRIC["YoY"]
    return list(dict.fromkeys(out))

def _build_query(metric, anomaly_text):
    bucket = _bucketize_metric(metric)
    extras = _EXTRAS_BY_BUCKET.get(bucket, []) + _GENERIC_EXTRAS
    syns   = _metric_synonyms(metric)
    dirs   = _direction_terms(anomaly_text)
    query = " ".join([metric, anomaly_text] + extras + syns + dirs)
    # ë„ˆë¬´ ê¸¸ë©´ ì•ìª½ í•µì‹¬ë§Œ ìœ ì§€
    return " ".join(query.split()[:120])

# ---------------- í”„ë¡¬í”„íŠ¸ ----------------
_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ ìƒì¥ì‚¬ì˜ ë¹„ì¬ë¬´ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¡œ 'ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ì˜ ë°°ê²½'ì„ ì„¤ëª…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
- ì•„ë˜ [CONTEXT]ì˜ ë¬¸ì¥ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”(ì¶”ì¸¡ ê¸ˆì§€).
- ê·¼ê±°ëŠ” ì„œë¡œ ë‹¤ë¥¸ source_idxë¥¼ ìš°ì„ í•˜ë©°, ê°€ëŠ¥í•œ í•œ 4~8ê°œ ì œì‹œí•˜ì„¸ìš”(í’ˆì§ˆ ë‚®ìœ¼ë©´ 4ê°œ ì´ìƒ).
- evidence í•­ëª©ì—ëŠ” ë°˜ë“œì‹œ source_idx, rcept_no, chunk_id, snippetì„ ë„£ìœ¼ì„¸ìš”.
- confidenceëŠ” 0.00~1.00 ë²”ìœ„ë¡œ, ê·¼ê±°ì˜ ì§ì ‘ì„±/ì¼ê´€ì„±/ì–‘ì„ ë°˜ì˜í•˜ì„¸ìš”.
- ì¶œë ¥ì€ JSON í•˜ë‚˜ë§Œ ë°˜í™˜(ì„¤ëª…/ì½”ë“œë¸”ë¡ ê¸ˆì§€).
"""

def _user_prompt(metric, anomaly_text, corp_code, quarter, query_text):
    return f"""
[Task]
ë‹¤ìŒ ì¬ë¬´ì§€í‘œ ì´ìƒì¹˜ì˜ ë¹„ì¬ë¬´ì  ë°°ê²½ì„ [CONTEXT]ì—ì„œ ì°¾ì•„ JSONìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

[ì…ë ¥]
- corp_code: {corp_code}
- quarter: {quarter}
- metric: {metric}
- anomaly: {anomaly_text}
- query_text: {query_text}

[ì¶œë ¥ JSON ìŠ¤í‚¤ë§ˆ]
{{
  "metric": "{metric}",
  "anomaly_text": "{anomaly_text}",
  "quarter": "{quarter}",
  "explanation_ko": "í•µì‹¬ ì›ì¸ ìš”ì•½(ìµœëŒ€ 5ë¬¸ì¥, 500ì ì´ë‚´)",
  "drivers": ["í•µì‹¬ ìš”ì¸ 1", "í•µì‹¬ ìš”ì¸ 2", "í•µì‹¬ ìš”ì¸ 3"],
  "confidence": 0.00,
  "evidence": [
    {{"source_idx": 1, "rcept_no":"...", "chunk_id":"...", "snippet":"..."}}
  ]
}}

[CONTEXT]
(ê° ë¸”ë¡ì˜ [n] ë²ˆí˜¸ëŠ” evidence.source_idxì— ëŒ€ì‘)
"""

# ---------------- ì»¨í…ìŠ¤íŠ¸/í¬ìŠ¤íŠ¸í”„ë¡œì„¸ì‹± ----------------
def _format_context_from_docs(docs):
    parts = []
    for i, d in enumerate(docs, start=1):
        meta = d.metadata or {}
        head = f"[{i}] quarter={meta.get('quarter','')} rcept_no={meta.get('rcept_no','')} chunk_id={meta.get('chunk_id','')}"
        if meta.get("report_nm") or meta.get("url"):
            head += f" title={meta.get('report_nm','')} url={meta.get('url','')}"
        body = (d.page_content or "").strip()
        parts.append(head + "\n" + body)
    return "\n\n---\n\n".join(parts)

def _extract_json(raw, fallback, verbose=True):
    try:
        return json.loads(raw)
    except Exception:
        s, e = raw.find("{"), raw.rfind("}")
        if s != -1 and e != -1 and e > s:
            try:
                return json.loads(raw[s:e+1])
            except Exception:
                pass
    _log("âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´", verbose)
    return fallback

def _clamp_conf(x, default=0.5):
    try: v = float(x)
    except Exception: v = default
    return max(0.0, min(1.0, v))

def _pack_evidence(ev_items, docs, limit=8):
    out = []
    for ev in (ev_items or [])[:limit]:
        idx = ev.get("source_idx")
        didx = idx-1 if isinstance(idx, int) else 0
        if not docs:
            meta = {}
        else:
            if 0 <= didx < len(docs): meta = docs[didx].metadata or {}
            else: meta = docs[0].metadata or {}
        out.append({
            "source_idx": idx if isinstance(idx, int) else 1,
            "rcept_no": meta.get("rcept_no",""),
            "chunk_id": meta.get("chunk_id",""),
            "snippet": (ev.get("snippet") or "")[:SNIPPET_MAX],
        })
    return out

# ---------------- í•µì‹¬: í•œ ì§€í‘œ ì²˜ë¦¬ ----------------
def explain_one_metric_with_llm(llm, retriever_fn, corp_code, quarter, metric, anomaly_text, top_k=TOP_K_DEFAULT, verbose=True):
    _log(f"ğŸ”¹ ì§€í‘œ ì²˜ë¦¬ ì‹œì‘: {metric}", verbose)
    t0 = time.perf_counter()

    query_text = _build_query(metric, anomaly_text)
    _log(f"   â–¸ ê²€ìƒ‰ ì¿¼ë¦¬: {query_text[:160]}", verbose)

    # ë²¡í„° ê²€ìƒ‰ + MMR ë¦¬ë­í¬(ë¦¬íŠ¸ë¦¬ë²„ ë‚´ë¶€)
    docs = retriever_fn(query_text)
    if not docs:
        _log("   â–¸ í›„ë³´ ë¬¸ë§¥ ì—†ìŒ â†’ ì •ë³´ ë¶€ì¡±", verbose)
        return {
            "metric": metric, "anomaly_text": anomaly_text, "quarter": quarter,
            "explanation_ko": "í•´ë‹¹ ë¶„ê¸° í…ìŠ¤íŠ¸ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "drivers": ["ì •ë³´ ë¶€ì¡±"], "confidence": 0.30, "evidence": []
        }

    # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = _format_context_from_docs(docs)
    messages = [
        {"role": "system", "content": _SYSTEM_PROMPT},
        {"role": "user", "content": _user_prompt(metric, anomaly_text, corp_code, quarter, query_text) + "\n" + context},
    ]

    _log("   â–¸ LLM í˜¸ì¶œâ€¦", verbose)
    t1 = time.perf_counter()
    try:
        raw = llm.invoke(messages).content
    except Exception as e:
        _log(f"   â–¸ LLM ì˜¤ë¥˜: {e}", verbose)
        raw = ""
    _log(f"   â–¸ LLM ì‘ë‹µ ìˆ˜ì‹  ({(time.perf_counter()-t1):.2f}s)", verbose)

    fallback = {
        "metric": metric, "anomaly_text": anomaly_text, "quarter": quarter,
        "explanation_ko": "ëª¨ë¸ ì‘ë‹µ ì˜¤ë¥˜ë¡œ ìš”ì•½ ë¶ˆê°€. ì¦ê±° ë¸”ë¡ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.",
        "drivers": ["ì •ë³´ ë¶€ì¡±"], "confidence": 0.30, "evidence": []
    }
    data = _extract_json(raw, fallback, verbose=verbose)

    # í•„ë“œ ë³´ì •
    explanation = (data.get("explanation_ko") or "").strip()[:500]
    drivers = [d.strip() for d in (data.get("drivers") or []) if isinstance(d, str) and d.strip()][:6]
    conf = _clamp_conf(data.get("confidence", 0.5))
    evidence = _pack_evidence(data.get("evidence") or [], docs, limit=8)

    _log(f"âœ… ì™„ë£Œ: {metric} / conf={conf:.2f} / evidence={len(evidence)}ê°œ / {(time.perf_counter()-t0):.2f}s", verbose)
    return {
        "metric": metric,
        "anomaly_text": anomaly_text,
        "quarter": quarter,
        "explanation_ko": explanation or "ê·¼ê±°ëŠ” ìˆìœ¼ë‚˜ ìš”ì•½ì´ ì¶©ë¶„ì¹˜ ì•ŠìŠµë‹ˆë‹¤.",
        "drivers": drivers or ["ì¦ê±° ê¸°ë°˜ ìš”ì•½ ë¶€ì¡±"],
        "confidence": conf,
        "evidence": evidence,
    }

# ---------------- ì „ì²´ íŒŒì´í”„ë¼ì¸ ----------------
def run_anomaly_explainer_min(anomalies_json_or_dict, corp_code, script_dir, quarter=None, model=CHAT_MODEL, temperature=0.2, top_k=TOP_K_DEFAULT, verbose=True):
    _log("ğŸš€ ì‹œì‘: ì´ìƒì¹˜ ê·¼ê±° ìƒì„± íŒŒì´í”„ë¼ì¸", verbose)

    # ì…ë ¥ íŒŒì‹±
    if isinstance(anomalies_json_or_dict, dict):
        anomalies = anomalies_json_or_dict
        _log(f"ğŸ§¾ ì…ë ¥: dict({len(anomalies)})", verbose)
    else:
        if os.path.exists(anomalies_json_or_dict):
            _log(f"ğŸ§¾ ì…ë ¥ íŒŒì¼ ë¡œë“œ: {anomalies_json_or_dict}", verbose)
            with open(anomalies_json_or_dict, "r", encoding="utf-8") as f:
                anomalies = json.load(f)
        else:
            _log("ğŸ§¾ ì…ë ¥ JSON ë¬¸ìì—´ íŒŒì‹±", verbose)
            anomalies = json.loads(anomalies_json_or_dict)

    # ê²½ë¡œ/DB/ë¶„ê¸°
    corp_root = _corp_root(script_dir, corp_code)
    db_path = _db_path(corp_root)
    conn = _connect(db_path, verbose=verbose)
    try:
        q = quarter or _latest_quarter(conn, corp_code, verbose=verbose)
        if not q:
            raise RuntimeError("ìµœì‹  ë¶„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. main.pyë¡œ ì¸ë±ì‹±ì„ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.")
    finally:
        conn.close()

    # ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ + ë¦¬íŠ¸ë¦¬ë²„(MMR)
    vs = build_or_load_faiss(corp_root)
    if vs is None:
        raise RuntimeError(f"FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ì‹±ì„ ìˆ˜í–‰í•˜ì„¸ìš”: {os.path.join(corp_root, 'index', 'faiss_index')}")
    retriever = get_retriever(vs, quarter=q, top_k=top_k)

    # LLM
    if not OPENAI_API_KEY:
        _log("âš ï¸  OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env í™•ì¸ í•„ìš”", verbose)
    _log(f"ğŸ§  LLM ì´ˆê¸°í™”: model={model}, temp={temperature}", verbose)
    llm = ChatOpenAI(model=model, temperature=temperature, timeout=120, max_retries=3, api_key=OPENAI_API_KEY)

    # ì§€í‘œ ë£¨í”„
    results = []
    _log(f"ğŸ” ì§€í‘œ ì²˜ë¦¬ ê°œìˆ˜: {len(anomalies)})", verbose)
    for i, (metric, text) in enumerate(anomalies.items(), start=1):
        _log(f"\nâ€”â€”â€” [{i}/{len(anomalies)}] {metric} â€”â€”â€”", verbose)
        item = explain_one_metric_with_llm(
            llm=llm,
            retriever_fn=retriever,
            corp_code=corp_code,
            quarter=q,
            metric=metric,
            anomaly_text=text,
            top_k=top_k,
            verbose=verbose,
        )
        results.append(item)

    out = {"corp_code": corp_code, "quarter": q, "results": results}
    out_dir = os.path.join(corp_root, "anomaly_explanations")
    out_path = os.path.join(out_dir, f"{q}.json")
    _save_json(out_path, out, verbose=verbose)
    _log("ğŸ‰ ì „ì²´ ì™„ë£Œ", verbose)
    return out


